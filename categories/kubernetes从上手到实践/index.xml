<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Kubernetes从上手到实践 on </title>
    <link>http://yipsen.github.io/categories/kubernetes%E4%BB%8E%E4%B8%8A%E6%89%8B%E5%88%B0%E5%AE%9E%E8%B7%B5/</link>
    <description>Recent content in Kubernetes从上手到实践 on </description>
    <generator>Hugo -- gohugo.io</generator>
    <lastBuildDate>Wed, 22 Dec 2021 01:47:51 +0800</lastBuildDate><atom:link href="http://yipsen.github.io/categories/kubernetes%E4%BB%8E%E4%B8%8A%E6%89%8B%E5%88%B0%E5%AE%9E%E8%B7%B5/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>24 总结</title>
      <link>http://yipsen.github.io/documents/columns/kubernetes/kubernetes%E4%BB%8E%E4%B8%8A%E6%89%8B%E5%88%B0%E5%AE%9E%E8%B7%B5/24-%E6%80%BB%E7%BB%93/</link>
      <pubDate>Wed, 22 Dec 2021 01:47:51 +0800</pubDate>
      
      <guid>http://yipsen.github.io/documents/columns/kubernetes/kubernetes%E4%BB%8E%E4%B8%8A%E6%89%8B%E5%88%B0%E5%AE%9E%E8%B7%B5/24-%E6%80%BB%E7%BB%93/</guid>
      <description>快速回顾 经过了前面 23 节的内容，我们从 K8S 的基础概念入手，通过其基础架构了解到了 K8S 中所涉及到的各类组件。
通过动手实践，使用 minikube 搭建了本地的集群，使用 kubeadm 完成了服务器上的集群搭建，对 K8S 的部署有了更加清晰的认识。
这里再推荐另一种正在快速迭代的方式 Kubernetes In Docker 可以很方便的创建廉价的 K8S 集群，目前至支持单节点集群，多节点支持正在开发中。
后面，我们通过学习 kubectl 的使用，部署了 Redis 服务，了解到了一个服务在 K8S 中部署的操作，以及如何将服务暴露至集群外，以便访问。
当集群真正要被使用之前，权限管控也愈发重要，我们通过学习 RBAC 的相关知识，学习到了如何在 K8S 集群中创建权限可控的用户，而这部分的内容在后续小节中也被频繁用到。
接下来，我们以我们实际的一个项目 SayThx 为例，一步步的完成了项目的部署，在此过程中也学习到了配置文件的编写规范和要求。
当项目变大时，维护项目的更新也变成了一件很麻烦的事情。由此，我们引入了 Helm 作为我们的包管理软件，并使用它进行了项目的部署。
在此过程中也学习到了 Helm 的架构，以及如何编写一个 Chart 等知识。
前面我们主要集中于如何使用 K8S 上，接下了庖丁解牛系列便带我们一同深入至 K8S 内部，了解到了各基础组件的实际工作原理，也深入到了源码内部，了解其实现逻辑。
有这些理论知识作为基础，我们便可以大胆的将应用部署至 K8S 之上了。但实际环境可能多种多样，你可以会遇到各种各样的问题。
这里我们介绍了一些常见的 Troubleshoot 的方法，以便你在后续使用 K8S 的过程中遇到问题也可以快速的定位并解决问题。
此外，我们学习了 K8S 的一些扩展，比如 Dashboard 和 CoreDNS ， Dashboard 是一个比较直观的管理资源的方式，它也还在快速的发展和迭代中。
CoreDNS 在 K8S 1.</description>
    </item>
    
    <item>
      <title>23 监控实践：对 K8S 集群进行监控</title>
      <link>http://yipsen.github.io/documents/columns/kubernetes/kubernetes%E4%BB%8E%E4%B8%8A%E6%89%8B%E5%88%B0%E5%AE%9E%E8%B7%B5/23-%E7%9B%91%E6%8E%A7%E5%AE%9E%E8%B7%B5%E5%AF%B9-k8s-%E9%9B%86%E7%BE%A4%E8%BF%9B%E8%A1%8C%E7%9B%91%E6%8E%A7/</link>
      <pubDate>Wed, 22 Dec 2021 01:47:50 +0800</pubDate>
      
      <guid>http://yipsen.github.io/documents/columns/kubernetes/kubernetes%E4%BB%8E%E4%B8%8A%E6%89%8B%E5%88%B0%E5%AE%9E%E8%B7%B5/23-%E7%9B%91%E6%8E%A7%E5%AE%9E%E8%B7%B5%E5%AF%B9-k8s-%E9%9B%86%E7%BE%A4%E8%BF%9B%E8%A1%8C%E7%9B%91%E6%8E%A7/</guid>
      <description>整体概览 通过前面的学习，我们对 K8S 有了一定的了解，也具备了一定的集群管理和排错能力。但如果要应用于生产环境中，不可能随时随地的都盯着集群，我们需要扩展我们对集群的感知能力。
本节，我们将介绍下 K8S 集群监控相关的内容。
监控什么 除去 K8S 外，我们平时自己开发的系统或者负责的项目，一般都是有监控的。监控可以提升我们的感知能力，便于我们及时了解集群的变化，以及知道哪里出现了问题。
K8S 是一个典型的分布式系统，组件很多，那么监控的目标，就变的很重要了。
总体来讲，对 K8S 集群的监控的话，主要有以下方面：
 节点情况 K8S 集群自身状态 部署在 K8S 内的应用的状态  Prometheus 对于 K8S 的监控，我们选择 CNCF 旗下次于 K8S 毕业的项目 Prometheus 。
Prometheus 是一个非常灵活易于扩展的监控系统，它通过各种 exporter 暴露数据，并由 prometheus server 定时去拉数据，然后存储。
它自己提供了一个简单的前端界面，可在其中使用 PromQL 的语法进行查询，并进行图形化展示。
安装 Prometheus  这里推荐一个项目 Prometheus Operator, 尽管该项目还处于 Beta 阶段，但是它给在 K8S 中搭建基于 Prometheus 的监控提供了很大的便利。
 我们此处选择以一般的方式进行部署，带你了解其整体的过程。
  创建一个独立的 Namespace：
apiVersion: v1kind: Namespacemetadata:name: monitoring# 将文件保存为 namespace.</description>
    </item>
    
    <item>
      <title>22 服务增强：Ingress</title>
      <link>http://yipsen.github.io/documents/columns/kubernetes/kubernetes%E4%BB%8E%E4%B8%8A%E6%89%8B%E5%88%B0%E5%AE%9E%E8%B7%B5/22-%E6%9C%8D%E5%8A%A1%E5%A2%9E%E5%BC%BAingress/</link>
      <pubDate>Wed, 22 Dec 2021 01:47:49 +0800</pubDate>
      
      <guid>http://yipsen.github.io/documents/columns/kubernetes/kubernetes%E4%BB%8E%E4%B8%8A%E6%89%8B%E5%88%B0%E5%AE%9E%E8%B7%B5/22-%E6%9C%8D%E5%8A%A1%E5%A2%9E%E5%BC%BAingress/</guid>
      <description>整体概览 通过前面的学习，我们已经知道 K8S 中有 Service 的概念，同时默认情况下还有 CoreDNS 完成集群内部的域名解析等工作，以此完成基础的服务注册发现能力。
在第 7 节中，我们介绍了 Service 的 4 种基础类型，在前面的介绍中，我们一般都在使用 ClusterIP 或 NodePort 等方式将服务暴露在集群内或者集群外。
本节，我们将介绍另一种处理服务访问的方式 Ingress。
Ingress 是什么 通过 kubectl explain ingress 命令，我们来看下对 Ingress 的描述。
 Ingress is a collection of rules that allow inbound connections to reach the endpoints defined by a backend. An Ingress can be configured to give services externally-reachable urls, load balance traffic, terminate SSL, offer name based virtual hosting etc.</description>
    </item>
    
    <item>
      <title>21 扩展增强：CoreDNS</title>
      <link>http://yipsen.github.io/documents/columns/kubernetes/kubernetes%E4%BB%8E%E4%B8%8A%E6%89%8B%E5%88%B0%E5%AE%9E%E8%B7%B5/21-%E6%89%A9%E5%B1%95%E5%A2%9E%E5%BC%BAcoredns/</link>
      <pubDate>Wed, 22 Dec 2021 01:47:48 +0800</pubDate>
      
      <guid>http://yipsen.github.io/documents/columns/kubernetes/kubernetes%E4%BB%8E%E4%B8%8A%E6%89%8B%E5%88%B0%E5%AE%9E%E8%B7%B5/21-%E6%89%A9%E5%B1%95%E5%A2%9E%E5%BC%BAcoredns/</guid>
      <description>整体概览 通过前面的学习，我们知道在 K8S 中有一套默认的集群内 DNS 服务，我们通常把它叫做 kube-dns，它基于 SkyDNS，为我们在服务注册发现方面提供了很大的便利。
比如，在我们的示例项目 SayThx 中，各组件便是依赖 DNS 进行彼此间的调用。
本节，我们将介绍的 CoreDNS 是 CNCF 旗下又一孵化项目，在 K8S 1.9 版本中加入并进入 Alpha 阶段。我们当前是以 K8S 1.11 的版本进行介绍，它并不是默认的 DNS 服务，但是它作为 K8S 的 DNS 插件的功能已经 GA 。
CoreDNS 在 K8S 1.13 版本中才正式成为默认的 DNS 服务。
CoreDNS 是什么 首先，我们需要明确 CoreDNS 是一个独立项目，它不仅可支持在 K8S 中使用，你也可以在你任何需要 DNS 服务的时候使用它。
CoreDNS 使用 Go 语言实现，部署非常方便。
它的扩展性很强，很多功能特性都是通过插件完成的，它不仅有大量的内置插件，同时也有很丰富的第三方插件。甚至你自己写一个插件也非常的容易。
如何安装使用 CoreDNS 我们这里主要是为了说明如何在 K8S 环境中使用它，所以对于独立安装部署它不做说明。
本小册中我们使用的是 K8S 1.11 版本，在第 5 小节 《搭建 Kubernetes 集群》中，我们介绍了使用 kubeadm 搭建集群。</description>
    </item>
    
    <item>
      <title>20 扩展增强：Dashboard</title>
      <link>http://yipsen.github.io/documents/columns/kubernetes/kubernetes%E4%BB%8E%E4%B8%8A%E6%89%8B%E5%88%B0%E5%AE%9E%E8%B7%B5/20-%E6%89%A9%E5%B1%95%E5%A2%9E%E5%BC%BAdashboard/</link>
      <pubDate>Wed, 22 Dec 2021 01:47:46 +0800</pubDate>
      
      <guid>http://yipsen.github.io/documents/columns/kubernetes/kubernetes%E4%BB%8E%E4%B8%8A%E6%89%8B%E5%88%B0%E5%AE%9E%E8%B7%B5/20-%E6%89%A9%E5%B1%95%E5%A2%9E%E5%BC%BAdashboard/</guid>
      <description>整体概览 通过前面的介绍，想必你已经迫不及待的想要将应用部署至 K8S 中，但总是使用 kubectl 或者 Helm 等命令行工具也许不太直观，你可能想要一眼就看到集群当前的状态，或者想要更方便的对集群进行管理。
本节将介绍一个 Web 项目 Dashboard 可用于部署容器化的应用程序，管理集群中的资源，甚至是排查和解决问题。
当然它和大多数 Dashboard 类的项目类似，也为集群的状态提供了一个很直观的展示。
如何安装 要想使用 Dashboard，首先我们需要安装它，而 Dashboard 的安装其实也很简单。不过对于国内用户需要注意的是需要解决网络问题，或替换镜像地址等。
这里我们安装当前最新版 v1.10.1 的 Dashboard：
  对于已经解决网络问题的用户：
可使用官方推荐做法进行安装，以下链接是使用了我提交了 path 的版本，由于官方最近的一次更新导致配置文件中的镜像搞错了。
master $ kubectl apply -f https://raw.githubusercontent.com/tao12345666333/dashboard/67970554aa9275cccec1d1ee5fbf89ae81b3b614/aio/deploy/recommended/kubernetes-dashboard.yamlsecret/kubernetes-dashboard-certs createdserviceaccount/kubernetes-dashboard createdrole.rbac.authorization.k8s.io/kubernetes-dashboard-minimal createdrolebinding.rbac.authorization.k8s.io/kubernetes-dashboard-minimal createddeployment.apps/kubernetes-dashboard createdservice/kubernetes-dashboard created  也可使用我修改过的这份（使用 Docker Hub 同步了镜像）仓库地址 GitHub, 国内 Gitee：
master $ kubectl apply -f https://gitee.com/K8S-release/k8s-dashboard/raw/master/kubernetes-dashboard.yamlsecret/kubernetes-dashboard-certs createdserviceaccount/kubernetes-dashboard createdrole.rbac.authorization.k8s.io/kubernetes-dashboard-minimal createdrolebinding.</description>
    </item>
    
    <item>
      <title>19 Troubleshoot</title>
      <link>http://yipsen.github.io/documents/columns/kubernetes/kubernetes%E4%BB%8E%E4%B8%8A%E6%89%8B%E5%88%B0%E5%AE%9E%E8%B7%B5/19-troubleshoot/</link>
      <pubDate>Wed, 22 Dec 2021 01:47:45 +0800</pubDate>
      
      <guid>http://yipsen.github.io/documents/columns/kubernetes/kubernetes%E4%BB%8E%E4%B8%8A%E6%89%8B%E5%88%B0%E5%AE%9E%E8%B7%B5/19-troubleshoot/</guid>
      <description>整体概览 通过前面的介绍，我们已经了解到了 K8S 的基础知识，核心组件原理以及如何在 K8S 中部署服务及管理服务等。
但在生产环境中，我们所面临的环境多种多样，可能会遇到各种问题。本节将结合我们已经了解到的知识，介绍一些常见问题定位和解决的思路或方法，以便大家在生产中使用 K8S 能如鱼得水。
应用部署问题 首先我们从应用部署相关的问题来入手。这里仍然使用我们的示例项目 SayThx。
clone 该项目，进入到 deploy 目录中，先 kubectl apply -f namespace.yaml 或者 kubectl create ns work 来创建一个用于实验的 Namespace 。
使用 describe 排查问题 对 redis-deployment.yaml 稍作修改，按以下方式操作：
master $ kubectl apply -f redis-deployment.yamldeployment.apps/saythx-redis createdmaster $ kubectl -n work get allNAME READY STATUS RESTARTS AGEpod/saythx-redis-7574c98f5d-v66fx 0/1 ImagePullBackOff 0 9sNAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGEdeployment.apps/saythx-redis 1 1 1 0 9sNAME DESIRED CURRENT READY AGEreplicaset.</description>
    </item>
    
    <item>
      <title>18 庖丁解牛：Container Runtime （Docker）</title>
      <link>http://yipsen.github.io/documents/columns/kubernetes/kubernetes%E4%BB%8E%E4%B8%8A%E6%89%8B%E5%88%B0%E5%AE%9E%E8%B7%B5/18-%E5%BA%96%E4%B8%81%E8%A7%A3%E7%89%9Bcontainer-runtime-docker/</link>
      <pubDate>Wed, 22 Dec 2021 01:47:44 +0800</pubDate>
      
      <guid>http://yipsen.github.io/documents/columns/kubernetes/kubernetes%E4%BB%8E%E4%B8%8A%E6%89%8B%E5%88%B0%E5%AE%9E%E8%B7%B5/18-%E5%BA%96%E4%B8%81%E8%A7%A3%E7%89%9Bcontainer-runtime-docker/</guid>
      <description>整体概览 我们在第 3 节的时候，提到过 Container Runtime 的概念，也大致介绍过它的主要作用在于下载镜像，运行容器等。
经过我们前面的学习，kube-scheduler 决定了 Pod 将被调度到哪个 Node 上，而 kubelet 则负责 Pod 在此 Node 上可按预期工作。如果没有 Container Runtime，那 Pod 中的 container 在该 Node 上也便无法正常启动运行了。
本节中，我们以当前最为通用的 Container Runtime Docker 为例进行介绍。
Container Runtime 是什么 Container Runtime 我们通常叫它容器运行时，而这一概念的产生也是由于容器化技术和 K8S 的大力发展，为了统一工业标准，也为了避免 K8S 绑定于特定的容器运行时，所以便成立了 Open Container Initiative (OCI) 组织，致力于将容器运行时标准化和容器镜像标准化。
凡是遵守此标准的实现，均可由标准格式的镜像启动相应的容器，并完成一些特定的操作。
Docker 是什么 Docker 是一个容器管理平台，它最初是被设计用于快速创建，发布和运行容器的工具，不过随着它的发展，其中集成了越来越多的功能。
Docker 也可以说是一个包含标准容器运行时的工具集，当前版本中默认的 runtime 称之为 runc。 关于 runc 相关的一些内容可参考我之前的一篇文章。
当然，这里提到了 默认的运行时 那也就意味着它可支持其他的运行时实现。
CRI 是什么 说到这里，我们就会发现，K8S 作为目前云原生技术体系中最重要的一环，为了让它更有扩展性，当然也不会将自己完全局限于某一种特定的容器运行时。
自 K8S 1.5 （2016 年 11 月）开始，新增了一个容器运行时的插件 API，并称之为 CRI （Container Runtime Interface），通过 CRI 可以支持 kubelet 使用不同的容器运行时，而不需要重新编译。</description>
    </item>
    
    <item>
      <title>17 庖丁解牛：kube-proxy</title>
      <link>http://yipsen.github.io/documents/columns/kubernetes/kubernetes%E4%BB%8E%E4%B8%8A%E6%89%8B%E5%88%B0%E5%AE%9E%E8%B7%B5/17-%E5%BA%96%E4%B8%81%E8%A7%A3%E7%89%9Bkube-proxy/</link>
      <pubDate>Wed, 22 Dec 2021 01:47:43 +0800</pubDate>
      
      <guid>http://yipsen.github.io/documents/columns/kubernetes/kubernetes%E4%BB%8E%E4%B8%8A%E6%89%8B%E5%88%B0%E5%AE%9E%E8%B7%B5/17-%E5%BA%96%E4%B8%81%E8%A7%A3%E7%89%9Bkube-proxy/</guid>
      <description>整体概览 在第 3 节中，我们了解到 kube-proxy 的存在，而在第 7 中，我们学习到了如何将运行于 K8S 中的服务以 Service 的方式暴露出来，以供访问。
本节，我们来介绍下 kube-proxy 了解下它是如何支撑起这种类似服务发现和代理相关功能的。
kube-proxy 是什么 kube-proxy 是 K8S 运行于每个 Node 上的网络代理组件，提供了 TCP 和 UDP 的连接转发支持。
我们已经知道，当 Pod 在创建和销毁的过程中，IP 可能会发生变化，而这就容易造成对其有依赖的服务的异常，所以通常情况下，我们都会使用 Service 将后端 Pod 暴露出来，而 Service 则较为稳定。
还是以我们之前的 SayThx 项目为例，但我们只部署其中没有任何依赖的后端资源 Redis 。
master $ git clone https://github.com/tao12345666333/saythx.gitCloning into &#39;saythx&#39;...remote: Enumerating objects: 110, done.remote: Counting objects: 100% (110/110), done.remote: Compressing objects: 100% (82/82), done.remote: Total 110 (delta 27), reused 102 (delta 20), pack-reused 0Receiving objects: 100% (110/110), 119.</description>
    </item>
    
    <item>
      <title>16 庖丁解牛：kubelet</title>
      <link>http://yipsen.github.io/documents/columns/kubernetes/kubernetes%E4%BB%8E%E4%B8%8A%E6%89%8B%E5%88%B0%E5%AE%9E%E8%B7%B5/16-%E5%BA%96%E4%B8%81%E8%A7%A3%E7%89%9Bkubelet/</link>
      <pubDate>Wed, 22 Dec 2021 01:47:42 +0800</pubDate>
      
      <guid>http://yipsen.github.io/documents/columns/kubernetes/kubernetes%E4%BB%8E%E4%B8%8A%E6%89%8B%E5%88%B0%E5%AE%9E%E8%B7%B5/16-%E5%BA%96%E4%B8%81%E8%A7%A3%E7%89%9Bkubelet/</guid>
      <description>整体概览 +--------------------------------------------------------+ | +---------------------+ +---------------------+ | | | kubelet | | kube-proxy | | | | | | | | | +---------------------+ +---------------------+ | | +----------------------------------------------------+ | | | Container Runtime (Docker) | | | | +---------------------+ +---------------------+ | | | | |Pod | |Pod | | | | | | +-----+ +-----+ | |+-----++-----++-----+| | | | | | |C1 | |C2 | | ||C1 ||C2 ||C3 || | | | | | | | | | | || || || || | | | | | +-----+ +-----+ | |+-----++-----++-----+| | | | | +---------------------+ +---------------------+ | | | +----------------------------------------------------+ | +--------------------------------------------------------+ 在第 3 节《宏观认识：整体架构》 中，我们知道了 K8S 中 Node 由一些必要的组件构成，而其中最为核心的当属 kubelet 了，如果没有 kubelet 的存在，那我们预期的各类资源就只能存在于 Master 的相关组件中了，而 K8S 也很能只是一个 CRUD 的普通程序了。本节，我们来介绍下 kubelet 及它是如何完成这一系列任务的。</description>
    </item>
    
    <item>
      <title>15 庖丁解牛：kube-scheduler</title>
      <link>http://yipsen.github.io/documents/columns/kubernetes/kubernetes%E4%BB%8E%E4%B8%8A%E6%89%8B%E5%88%B0%E5%AE%9E%E8%B7%B5/15-%E5%BA%96%E4%B8%81%E8%A7%A3%E7%89%9Bkube-scheduler/</link>
      <pubDate>Wed, 22 Dec 2021 01:47:41 +0800</pubDate>
      
      <guid>http://yipsen.github.io/documents/columns/kubernetes/kubernetes%E4%BB%8E%E4%B8%8A%E6%89%8B%E5%88%B0%E5%AE%9E%E8%B7%B5/15-%E5%BA%96%E4%B8%81%E8%A7%A3%E7%89%9Bkube-scheduler/</guid>
      <description>整体概览 +----------------------------------------------------------+ | Master | | +-------------------------+ | | +-------&amp;gt;| API Server |&amp;lt;--------+ | | | | | | | | v +-------------------------+ v | | +----------------+ ^ +--------------------+ | | | | | | | | | | Scheduler | | | Controller Manager | | | | | | | | | | +----------------+ v +--------------------+ | | +------------------------------------------------------+ | | | | | | | Cluster state store | | | | | | | +------------------------------------------------------+ | +----------------------------------------------------------+ 在第 3 节《宏观认识：整体架构》 中，我们也认识到了 Scheduler 的存在，知道了 Master 是 K8S 是集群的大脑，Controller Manager 负责将集群调整至预期的状态，而 Scheduler 则是集群调度器，将预期的 Pod 资源调度到正确的 Node 节点上，进而令该 Pod 可完成启动。本节我们一同来看看它如何发挥如此大的作用。</description>
    </item>
    
    <item>
      <title>14 庖丁解牛：controller-manager</title>
      <link>http://yipsen.github.io/documents/columns/kubernetes/kubernetes%E4%BB%8E%E4%B8%8A%E6%89%8B%E5%88%B0%E5%AE%9E%E8%B7%B5/14-%E5%BA%96%E4%B8%81%E8%A7%A3%E7%89%9Bcontroller-manager/</link>
      <pubDate>Wed, 22 Dec 2021 01:47:40 +0800</pubDate>
      
      <guid>http://yipsen.github.io/documents/columns/kubernetes/kubernetes%E4%BB%8E%E4%B8%8A%E6%89%8B%E5%88%B0%E5%AE%9E%E8%B7%B5/14-%E5%BA%96%E4%B8%81%E8%A7%A3%E7%89%9Bcontroller-manager/</guid>
      <description>整体概览 +----------------------------------------------------------+ | Master | | +-------------------------+ | | +-------&amp;gt;| API Server |&amp;lt;--------+ | | | | | | | | v +-------------------------+ v | | +----------------+ ^ +--------------------+ | | | | | | | | | | Scheduler | | | Controller Manager | | | | | | | | | | +----------------+ v +--------------------+ | | +------------------------------------------------------+ | | | | | | | Cluster state store | | | | | | | +------------------------------------------------------+ | +----------------------------------------------------------+ 在第 3 节《宏观认识：整体架构》 中，我们也认识到了 Controller Manager 的存在，知道了 Master 是 K8S 是集群的大脑，而它则是 Master 中最繁忙的部分。为什么这么说？本节我们一同来看看它为何如此繁忙。</description>
    </item>
    
    <item>
      <title>13 庖丁解牛：etcd</title>
      <link>http://yipsen.github.io/documents/columns/kubernetes/kubernetes%E4%BB%8E%E4%B8%8A%E6%89%8B%E5%88%B0%E5%AE%9E%E8%B7%B5/13-%E5%BA%96%E4%B8%81%E8%A7%A3%E7%89%9Betcd/</link>
      <pubDate>Wed, 22 Dec 2021 01:47:39 +0800</pubDate>
      
      <guid>http://yipsen.github.io/documents/columns/kubernetes/kubernetes%E4%BB%8E%E4%B8%8A%E6%89%8B%E5%88%B0%E5%AE%9E%E8%B7%B5/13-%E5%BA%96%E4%B8%81%E8%A7%A3%E7%89%9Betcd/</guid>
      <description>整体概览 +----------------------------------------------------------+ | Master | | +-------------------------+ | | +-------&amp;gt;| API Server |&amp;lt;--------+ | | | | | | | | v +-------------------------+ v | | +----------------+ ^ +--------------------+ | | | | | | | | | | Scheduler | | | Controller Manager | | | | | | | | | | +----------------+ v +--------------------+ | | +------------------------------------------------------+ | | | | | | | Cluster state store | | | | | | | +------------------------------------------------------+ | +----------------------------------------------------------+ 在第 3 节《宏观认识：整体架构》 中，我们也认识到了 etcd 的存在，知道了 Master 是 K8S 是集群的大脑，而 etcd 则是大脑的核心。为什么这么说？本节我们一同来看看 etcd 为何如此重要。</description>
    </item>
    
    <item>
      <title>12 庖丁解牛：kube-apiserver</title>
      <link>http://yipsen.github.io/documents/columns/kubernetes/kubernetes%E4%BB%8E%E4%B8%8A%E6%89%8B%E5%88%B0%E5%AE%9E%E8%B7%B5/12-%E5%BA%96%E4%B8%81%E8%A7%A3%E7%89%9Bkube-apiserver/</link>
      <pubDate>Wed, 22 Dec 2021 01:47:38 +0800</pubDate>
      
      <guid>http://yipsen.github.io/documents/columns/kubernetes/kubernetes%E4%BB%8E%E4%B8%8A%E6%89%8B%E5%88%B0%E5%AE%9E%E8%B7%B5/12-%E5%BA%96%E4%B8%81%E8%A7%A3%E7%89%9Bkube-apiserver/</guid>
      <description>整体概览 +----------------------------------------------------------+ | Master | | +-------------------------+ | | +-------&amp;gt;| API Server |&amp;lt;--------+ | | | | | | | | v +-------------------------+ v | | +----------------+ ^ +--------------------+ | | | | | | | | | | Scheduler | | | Controller Manager | | | | | | | | | | +----------------+ v +--------------------+ | | +------------------------------------------------------+ | | | | | | | Cluster state store | | | | | | | +------------------------------------------------------+ | +----------------------------------------------------------+ 在第 3 节《宏观认识：整体架构》 中，我们初次认识到了 kube-apiserver 的存在（以下内容中将统一称之为 kube-apiserver），知道了它作为集群的统一入口，接收来自外部的信号和请求，并将一些信息存储至 etcd 中。</description>
    </item>
    
    <item>
      <title>11 部署实践：以 Helm 部署项目</title>
      <link>http://yipsen.github.io/documents/columns/kubernetes/kubernetes%E4%BB%8E%E4%B8%8A%E6%89%8B%E5%88%B0%E5%AE%9E%E8%B7%B5/11-%E9%83%A8%E7%BD%B2%E5%AE%9E%E8%B7%B5%E4%BB%A5-helm-%E9%83%A8%E7%BD%B2%E9%A1%B9%E7%9B%AE/</link>
      <pubDate>Wed, 22 Dec 2021 01:47:37 +0800</pubDate>
      
      <guid>http://yipsen.github.io/documents/columns/kubernetes/kubernetes%E4%BB%8E%E4%B8%8A%E6%89%8B%E5%88%B0%E5%AE%9E%E8%B7%B5/11-%E9%83%A8%E7%BD%B2%E5%AE%9E%E8%B7%B5%E4%BB%A5-helm-%E9%83%A8%E7%BD%B2%E9%A1%B9%E7%9B%AE/</guid>
      <description>概览 上节，我们学习到了 Helm 的基础概念和工作原理，本节我们将 Helm 用于我们的实际项目，编写 Helm chart 以及通过 Helm 进行部署。
Helm chart 上节我们解释过 chart 的含义，现在我们要将项目使用 Helm 部署，那么首先，我们需要创建一个 chart。
Chart 结构 在我们项目的根目录下，通过以下命令创建一个 chart。
➜ saythx git:(master) helm create saythxCreating saythx➜ saythx git:(master) ✗ tree -a saythxsaythx├── charts├── Chart.yaml├── .helmignore├── templates│ ├── deployment.yaml│ ├── _helpers.tpl│ ├── ingress.yaml│ ├── NOTES.txt│ └── service.yaml└── values.yaml2 directories, 8 files创建完成后，我们可以看到默认创建的 chart 中包含了几个文件和目录。我们先对其进行解释。</description>
    </item>
    
    <item>
      <title>10 应用管理：初识 Helm</title>
      <link>http://yipsen.github.io/documents/columns/kubernetes/kubernetes%E4%BB%8E%E4%B8%8A%E6%89%8B%E5%88%B0%E5%AE%9E%E8%B7%B5/10-%E5%BA%94%E7%94%A8%E7%AE%A1%E7%90%86%E5%88%9D%E8%AF%86-helm/</link>
      <pubDate>Wed, 22 Dec 2021 01:47:36 +0800</pubDate>
      
      <guid>http://yipsen.github.io/documents/columns/kubernetes/kubernetes%E4%BB%8E%E4%B8%8A%E6%89%8B%E5%88%B0%E5%AE%9E%E8%B7%B5/10-%E5%BA%94%E7%94%A8%E7%AE%A1%E7%90%86%E5%88%9D%E8%AF%86-helm/</guid>
      <description>整体概览 上节，我们已经学习了如何通过编写配置文件的方式部署项目。而在实际生产环境中，项目所包含组件可能不止 3 个，并且可能项目数会很多，如果每个项目的发布，更新等都通过手动去编写配置文件的方式，实在不利于管理。
并且，当线上出现个别组件升级回滚之类的操作，如果组件之间有相关版本依赖等情况，那事情会变得复杂的多。我们需要有更简单的机制来辅助我们完成这些事情。
Helm 介绍 Helm 是构建于 K8S 之上的包管理器，可与我们平时接触到的 Yum，APT，Homebrew 或者 Pip 等包管理器相类比。
使用 Helm 可简化包分发，安装，版本管理等操作流程。同时它也是 CNCF 孵化项目。
Helm 安装 Helm 是 C/S 架构，主要分为客户端 helm 和服务端 Tiller。安装时可直接在 Helm 仓库的 Release 页面 下载所需二进制文件或者源码包。
由于当前项目的二进制文件存储已切换为 GCS，我已经为国内用户准备了最新版本的二进制包，可通过以下链接进行下载。
链接: https://pan.baidu.com/s/1n1zj3rlv2NyfiA6kRGrHfg 提取码: 5huw 下载后对文件进行解压，我这里以 Linux amd64 为例。
➜ /tmp tar -zxvf helm-v2.11.0-linux-amd64.tar.gzlinux-amd64/linux-amd64/tillerlinux-amd64/README.mdlinux-amd64/helmlinux-amd64/LICENSE➜ /tmp tree linux-amd64 linux-amd64├── helm├── LICENSE├── README.md└── tiller0 directories, 4 files解压完成后，可看到其中包含 helm 和 tiller 二进制文件。</description>
    </item>
    
    <item>
      <title>09 应用发布：部署实际项目</title>
      <link>http://yipsen.github.io/documents/columns/kubernetes/kubernetes%E4%BB%8E%E4%B8%8A%E6%89%8B%E5%88%B0%E5%AE%9E%E8%B7%B5/09-%E5%BA%94%E7%94%A8%E5%8F%91%E5%B8%83%E9%83%A8%E7%BD%B2%E5%AE%9E%E9%99%85%E9%A1%B9%E7%9B%AE/</link>
      <pubDate>Wed, 22 Dec 2021 01:47:35 +0800</pubDate>
      
      <guid>http://yipsen.github.io/documents/columns/kubernetes/kubernetes%E4%BB%8E%E4%B8%8A%E6%89%8B%E5%88%B0%E5%AE%9E%E8%B7%B5/09-%E5%BA%94%E7%94%A8%E5%8F%91%E5%B8%83%E9%83%A8%E7%BD%B2%E5%AE%9E%E9%99%85%E9%A1%B9%E7%9B%AE/</guid>
      <description>本节我们开始学习如何将实际项目部署至 K8S 中，开启生产实践之路。
整体概览 本节所用示例项目是一个混合了 Go，NodeJS，Python 等语言的项目，灵感来自于知名程序员 Kenneth Reitz 的 Say Thanks 项目。本项目实现的功能主要有两个：1. 用户通过前端发送感谢消息 2. 有个工作进程会持续的计算收到感谢消息的排行榜。项目代码可在 GitHub 上获得。接下来几节中如果需要用到此项目我会统一称之为 saythx 项目。
saythx 项目的基础结构如下图：
构建镜像 前端 我们使用了前端框架 Vue，所以在做生产部署时，需要先在 Node JS 的环境下进行打包构建。包管理器使用的是 Yarn。然后使用 Nginx 提供服务，并进行反向代理，将请求正确的代理至后端。
FROM node:10.13 as builderWORKDIR /appCOPY . /appRUN yarn install \&amp;amp;&amp;amp; yarn buildFROM nginx:1.15COPY nginx.conf /etc/nginx/conf.d/default.confCOPY --from=builder /app/dist /usr/share/nginx/html/EXPOSE 80Nginx 的配置文件如下：
upstream backend-up {server saythx-backend:8080;}server {listen 80;server_name localhost;charset utf-8;location / {root /usr/share/nginx/html;try_files $uri $uri/ /index.</description>
    </item>
    
    <item>
      <title>08 安全重点 认证和授权</title>
      <link>http://yipsen.github.io/documents/columns/kubernetes/kubernetes%E4%BB%8E%E4%B8%8A%E6%89%8B%E5%88%B0%E5%AE%9E%E8%B7%B5/08-%E5%AE%89%E5%85%A8%E9%87%8D%E7%82%B9-%E8%AE%A4%E8%AF%81%E5%92%8C%E6%8E%88%E6%9D%83/</link>
      <pubDate>Wed, 22 Dec 2021 01:47:34 +0800</pubDate>
      
      <guid>http://yipsen.github.io/documents/columns/kubernetes/kubernetes%E4%BB%8E%E4%B8%8A%E6%89%8B%E5%88%B0%E5%AE%9E%E8%B7%B5/08-%E5%AE%89%E5%85%A8%E9%87%8D%E7%82%B9-%E8%AE%A4%E8%AF%81%E5%92%8C%E6%8E%88%E6%9D%83/</guid>
      <description>本节我们将开始学习将 K8S 应用于生产环境中至关重要的一环，权限控制。当然，不仅是 K8S 对于任何应用于生产环境中的系统，权限管理或者说访问控制都是很重要的。
整体概览 通过前面的学习，我们已经知道 K8S 中几乎所有的操作都需要经过 kube-apiserver 处理，所以为了安全起见，K8S 为它提供了三类安全访问的措施。分别是：用于识别用户身份的认证（Authentication），用于控制用户对资源访问的授权（Authorization）以及用于资源管理方面的准入控制（Admission Control）。
下面的图基本展示了这一过程。来自客户端的请求分别经过认证，授权，准入控制之后，才能真正执行。
当然，这里说基本展示是因为我们可以直接通过 kubectl proxy 的方式直接通过 HTTP 请求访问 kube-apiserver 而无需任何认证过程。
另外，也可通过在 kube-apiserver 所启动的机器上，直接访问启动时 --insecure-port 参数配置的端口进行绕过认证和授权，默认是 8080。为了避免安全问题，也可将此参数设置为 0 以规避问题。注意：这个参数和 --insecure-bind-address 都已过期，并将在未来的版本移除。
+-----------------------------------------------------------------------------------------------------------+| || +---------------------------------------------------------------------------+ +--------+ || | | | | || +--------+ | +------------------+ +----------------+ +--------------+ +------+ | | | || | | | | | | | | Admission | | | | | | || | Client +------&amp;gt; | Authentication +-&amp;gt; | Authorization +-&amp;gt; | Control +-&amp;gt; |Logic | +--&amp;gt; | Others | || | | | | | | | | | | | | | | || +--------+ | +------------------+ +----------------+ +--------------+ +------+ | | | || | | | | || | | | | || | Kube-apiserver | | | || +---------------------------------------------------------------------------+ +--------+ || |+-----------------------------------------------------------------------------------------------------------+认证（Authentication） 认证，无非是判断当前发起请求的用户身份是否正确。例如，我们通常登录服务器时候需要输入用户名和密码，或者 SSH Keys 之类的。</description>
    </item>
    
    <item>
      <title>07 集群管理：以 Redis 为例-部署及访问</title>
      <link>http://yipsen.github.io/documents/columns/kubernetes/kubernetes%E4%BB%8E%E4%B8%8A%E6%89%8B%E5%88%B0%E5%AE%9E%E8%B7%B5/07-%E9%9B%86%E7%BE%A4%E7%AE%A1%E7%90%86%E4%BB%A5-redis-%E4%B8%BA%E4%BE%8B-%E9%83%A8%E7%BD%B2%E5%8F%8A%E8%AE%BF%E9%97%AE/</link>
      <pubDate>Wed, 22 Dec 2021 01:47:33 +0800</pubDate>
      
      <guid>http://yipsen.github.io/documents/columns/kubernetes/kubernetes%E4%BB%8E%E4%B8%8A%E6%89%8B%E5%88%B0%E5%AE%9E%E8%B7%B5/07-%E9%9B%86%E7%BE%A4%E7%AE%A1%E7%90%86%E4%BB%A5-redis-%E4%B8%BA%E4%BE%8B-%E9%83%A8%E7%BD%B2%E5%8F%8A%E8%AE%BF%E9%97%AE/</guid>
      <description>上节我们已经学习了 kubectl 的基础使用，本节我们使用 kubectl 在 K8S 中进行部署。
前面我们已经说过，Pod 是 K8S 中最小的调度单元，所以我们无法直接在 K8S 中运行一个 container 但是我们可以运行一个 Pod 而这个 Pod 中只包含一个 container 。
从 kubectl run 开始 kubectl run 的基础用法如下：
Usage:kubectl run NAME --image=image [--env=&amp;quot;key=value&amp;quot;] [--port=port] [--replicas=replicas] [--dry-run=bool] [--overrides=inline-json] [--command] -- [COMMAND] [args...] [options]NAME 和 --image 是必需项。分别代表此次部署的名字及所使用的镜像，其余部分之后进行解释。当然，在我们实际使用时，推荐编写配置文件并通过 kubectl create 进行部署。
使用最小的 Redis 镜像 在 Redis 的官方镜像列表可以看到有很多的 tag 可供选择，其中使用 Alpine Linux 作为基础的镜像体积最小，下载较为方便。我们选择 redis:alpine 这个镜像进行部署。
部署 现在我们只部署一个 Redis 实例。
➜ ~ kubectl run redis --image=&#39;redis:alpine&#39;deployment.</description>
    </item>
    
    <item>
      <title>06 集群管理：初识 kubectl</title>
      <link>http://yipsen.github.io/documents/columns/kubernetes/kubernetes%E4%BB%8E%E4%B8%8A%E6%89%8B%E5%88%B0%E5%AE%9E%E8%B7%B5/06-%E9%9B%86%E7%BE%A4%E7%AE%A1%E7%90%86%E5%88%9D%E8%AF%86-kubectl/</link>
      <pubDate>Wed, 22 Dec 2021 01:47:32 +0800</pubDate>
      
      <guid>http://yipsen.github.io/documents/columns/kubernetes/kubernetes%E4%BB%8E%E4%B8%8A%E6%89%8B%E5%88%B0%E5%AE%9E%E8%B7%B5/06-%E9%9B%86%E7%BE%A4%E7%AE%A1%E7%90%86%E5%88%9D%E8%AF%86-kubectl/</guid>
      <description>从本节开始，我们来学习 K8S 集群管理相关的知识。通过前面的学习，我们知道 K8S 遵循 C/S 架构，官方也提供了 CLI 工具 kubectl 用于完成大多数集群管理相关的功能。当然凡是你可以通过 kubectl 完成的与集群交互的功能，都可以直接通过 API 完成。
对于我们来说 kubectl 并不陌生，在第 3 章讲 K8S 整体架构时，我们首次提到了它。在第 4 章和第 5 章介绍了两种安装 kubectl 的方式故而本章不再赘述安装的部分。
整体概览 首先我们在终端下执行下 kubectl:
➜ ~ kubectl kubectl controls the Kubernetes cluster manager....Usage:kubectl [flags] [options]kubectl 已经将命令做了基本的归类，同时显示了其一般的用法 kubectl [flags] [options] 。
使用 kubectl options 可以看到所有全局可用的配置项。
基础配置 在我们的用户家目录，可以看到一个名为 .kube/config 的配置文件，我们来看下其中的内容（此处以本地的 minikube 集群为例）。
➜ ~ ls $HOME/.kube/config /home/tao/.kube/config➜ ~ cat $HOME/.kube/configapiVersion: v1clusters:- cluster:certificate-authority: /home/tao/.</description>
    </item>
    
    <item>
      <title>05 动手实践：搭建一个 Kubernetes 集群 - 生产可用</title>
      <link>http://yipsen.github.io/documents/columns/kubernetes/kubernetes%E4%BB%8E%E4%B8%8A%E6%89%8B%E5%88%B0%E5%AE%9E%E8%B7%B5/05-%E5%8A%A8%E6%89%8B%E5%AE%9E%E8%B7%B5%E6%90%AD%E5%BB%BA%E4%B8%80%E4%B8%AA-kubernetes-%E9%9B%86%E7%BE%A4-%E7%94%9F%E4%BA%A7%E5%8F%AF%E7%94%A8/</link>
      <pubDate>Wed, 22 Dec 2021 01:47:31 +0800</pubDate>
      
      <guid>http://yipsen.github.io/documents/columns/kubernetes/kubernetes%E4%BB%8E%E4%B8%8A%E6%89%8B%E5%88%B0%E5%AE%9E%E8%B7%B5/05-%E5%8A%A8%E6%89%8B%E5%AE%9E%E8%B7%B5%E6%90%AD%E5%BB%BA%E4%B8%80%E4%B8%AA-kubernetes-%E9%9B%86%E7%BE%A4-%E7%94%9F%E4%BA%A7%E5%8F%AF%E7%94%A8/</guid>
      <description>通过上一节的学习，我们快速的使用 Minikube 搭建了一个本地可用的 K8S 集群。默认情况下，节点是一个虚拟机实例，我们可以在上面体验一些基本的功能。
大多数人的需求并不只是包含一个虚拟机节点的本地测试集群，而是一个可在服务器运行，可自行扩/缩容，具备全部功能的，达到生产可用的集群。
K8S 集群的搭建，一直让很多人头疼，本节我们来搭建一个生产可用的集群，便于后续的学习或使用。
方案选择 K8S 生产环境可用的集群方案有很多，本节我们选择一个 Kubernetes 官方推荐的方案 kubeadm 进行搭建。
kubeadm 是 Kubernetes 官方提供的一个 CLI 工具，可以很方便的搭建一套符合官方最佳实践的最小化可用集群。当我们使用 kubeadm 搭建集群时，集群可以通过 K8S 的一致性测试，并且 kubeadm 还支持其他的集群生命周期功能，比如升级/降级等。
我们在此处选择 kubeadm ，因为我们可以不用过于关注集群的内部细节，便可以快速的搭建出生产可用的集群。我们可以通过后续章节的学习，快速上手 K8S ，并学习到 K8S 的内部原理。在此基础上，想要在物理机上完全一步步搭建集群，便轻而易举。
安装基础组件 前期准备 使用 kubeadm 前，我们需要提前做一些准备。
  我们需要禁用 swap。通过之前的学习，我们知道每个节点上都有个必须的组件，名为 kubelet，自 K8S 1.8 开始，启动 kubelet 时，需要禁用 swap 。或者需要更改 kubelet 的启动参数 --fail-swap-on=false。
虽说可以更改参数让其可用，但是我建议还是禁用 swap 除非你的集群有特殊的需求，比如：有大内存使用的需求，但又想节约成本；或者你知道你将要做什么，否则可能会出现一些非预期的情况，尤其是做了内存限制的时候，当某个 Pod 达到内存限制的时候，它可能会溢出到 swap 中，这会导致 K8S 无法正常进行调度。
如何禁用：
 使用 sudo cat /proc/swaps 验证 swap 配置的设备和文件。 通过 swapoff -a 关闭 swap 。 使用 sudo blkid 或者 sudo lsblk 可查看到我们的设备属性，请注意输出结果中带有 swap 字样的信息。 将 /etc/fstab 中和上一条命令中输出的，和 swap 相关的挂载点都删掉，以免在机器重启或重挂载时，再挂载 swap 分区。  执行完上述操作，swap 便会被禁用，当然你也可以再次通过上述命令，或者 free 命令来确认是否还有 swap 存在。</description>
    </item>
    
    <item>
      <title>04 搭建 Kubernetes 集群 - 本地快速搭建</title>
      <link>http://yipsen.github.io/documents/columns/kubernetes/kubernetes%E4%BB%8E%E4%B8%8A%E6%89%8B%E5%88%B0%E5%AE%9E%E8%B7%B5/04-%E6%90%AD%E5%BB%BA-kubernetes-%E9%9B%86%E7%BE%A4-%E6%9C%AC%E5%9C%B0%E5%BF%AB%E9%80%9F%E6%90%AD%E5%BB%BA/</link>
      <pubDate>Wed, 22 Dec 2021 01:47:30 +0800</pubDate>
      
      <guid>http://yipsen.github.io/documents/columns/kubernetes/kubernetes%E4%BB%8E%E4%B8%8A%E6%89%8B%E5%88%B0%E5%AE%9E%E8%B7%B5/04-%E6%90%AD%E5%BB%BA-kubernetes-%E9%9B%86%E7%BE%A4-%E6%9C%AC%E5%9C%B0%E5%BF%AB%E9%80%9F%E6%90%AD%E5%BB%BA/</guid>
      <description>通过之前的学习，我们已经知道了 K8S 中有一些组件是必须的，集群中有不同的角色。本节，我们在本地快速搭建一个集群，以加深我们学习到的东西。
方案选择 在上一节中，我们知道 K8S 中有多种功能组件，而这些组件要在本地全部搭建好，需要一些基础知识，以及在搭建过程中会浪费不少的时间，从而可能会影响我们正常的搭建集群的目标。
所以，我们这里提供两个最简单，最容易实现我们目标的工具
 KIND 。 Minikube 。  KIND 介绍 KIND（Kubernetes in Docker）是为了能提供更加简单，高效的方式来启动 K8S 集群，目前主要用于比如 Kubernetes 自身的 CI 环境中。
安装  可以直接在项目的 Release 页面 下载已经编译好的二进制文件。(下文中使用的是 v0.1.0 版本的二进制包)  注意：如果不直接使用二进制包，而是使用 go get sigs.k8s.io/kind 的方式下载，则与下文中的配置文件不兼容。请参考使用 Kind 搭建你的本地 Kubernetes 集群 这篇文章。
更新（2020年2月5日）：KIND 已经发布了 v0.7.0 版本，如果你想使用新版本，建议参考 使用 Kind 在离线环境创建 K8S 集群 ，这篇文章使用了最新版本的 KIND。
创建集群 在使用 KIND 之前，你需要本地先安装好 Docker 的环境 ，此处暂不做展开。
由于网络问题，我们此处也需要写一个配置文件，以便让 kind 可以使用国内的镜像源。（KIND 最新版本中已经内置了所有需要的镜像，无需此操作）
apiVersion: kind.sigs.k8s.io/v1alpha1kind: ConfigkubeadmConfigPatches:- |apiVersion: kubeadm.</description>
    </item>
    
    <item>
      <title>03 宏观认识：整体架构</title>
      <link>http://yipsen.github.io/documents/columns/kubernetes/kubernetes%E4%BB%8E%E4%B8%8A%E6%89%8B%E5%88%B0%E5%AE%9E%E8%B7%B5/03-%E5%AE%8F%E8%A7%82%E8%AE%A4%E8%AF%86%E6%95%B4%E4%BD%93%E6%9E%B6%E6%9E%84/</link>
      <pubDate>Wed, 22 Dec 2021 01:47:29 +0800</pubDate>
      
      <guid>http://yipsen.github.io/documents/columns/kubernetes/kubernetes%E4%BB%8E%E4%B8%8A%E6%89%8B%E5%88%B0%E5%AE%9E%E8%B7%B5/03-%E5%AE%8F%E8%A7%82%E8%AE%A4%E8%AF%86%E6%95%B4%E4%BD%93%E6%9E%B6%E6%9E%84/</guid>
      <description>工欲善其事，必先利其器。本节我们来从宏观上认识下 K8S 的整体架构，以便于后续在此基础上进行探索和实践。
C/S 架构 从更高层来看，K8S 整体上遵循 C/S 架构，从这个角度来看，可用下面的图来表示其结构：
 +-------------+ | | | | +---------------+| | +-----&amp;gt; | Node 1 || Kubernetes | | +---------------++-----------------+ | Server | | | CLI | | | | +---------------+| (Kubectl) |-----------&amp;gt;| ( Master ) |&amp;lt;------+-----&amp;gt; | Node 2 || | | | | +---------------++-----------------+ | | | | | | +---------------+| | +-----&amp;gt; | Node 3 || | +---------------++-------------+ 左侧是一个官方提供的名为 kubectl 的 CLI （Command Line Interface）工具，用于使用 K8S 开放的 API 来管理集群和操作对象等。</description>
    </item>
    
    <item>
      <title>02 初步认识：Kubernetes 基础概念</title>
      <link>http://yipsen.github.io/documents/columns/kubernetes/kubernetes%E4%BB%8E%E4%B8%8A%E6%89%8B%E5%88%B0%E5%AE%9E%E8%B7%B5/02-%E5%88%9D%E6%AD%A5%E8%AE%A4%E8%AF%86kubernetes-%E5%9F%BA%E7%A1%80%E6%A6%82%E5%BF%B5/</link>
      <pubDate>Wed, 22 Dec 2021 01:47:28 +0800</pubDate>
      
      <guid>http://yipsen.github.io/documents/columns/kubernetes/kubernetes%E4%BB%8E%E4%B8%8A%E6%89%8B%E5%88%B0%E5%AE%9E%E8%B7%B5/02-%E5%88%9D%E6%AD%A5%E8%AE%A4%E8%AF%86kubernetes-%E5%9F%BA%E7%A1%80%E6%A6%82%E5%BF%B5/</guid>
      <description>好了，总算开始进入正题，抛弃掉死板的说教模式，我们以一个虚构的新成立的项目组为例开始我们的 Kubernetes 探索。(以下统一将 Kubernetes 简写为 K8S) 项目组目前就只有一个成员，我们称他为小张。项目组刚成立的时候，小张也没想好，具体要做什么，但肯定要对外提供服务的，所以先向公司申请了一台服务器。
Node 这台服务器可以用来做什么呢？跑服务，跑数据库，跑测试之类的都可以，我们将它所做的事情统称为工作(work) 那么，它便是工作节点 (worker Node) 对应于 K8S 中，这就是我们首先要认识的 Node 。
Node 可以是一台物理机，也可以是虚拟机，对于我们此处的项目来讲，这台服务器便是 K8S 中的 Node 。
Node 状态 当我们拿到这台服务器后，首先我们登录服务器查看下服务器的基本配置和信息。其实对于一个新加入 K8S 集群的 Node 也是一样，需要先检查它的状态，并将状态上报至集群的 master 。我们来看看服务器有哪些信息是我们所关心的。
地址 首先，我们所关心的是我们服务器的 IP 地址，包括内网 IP 和外网 IP。对应于 K8S 集群的话这个概念是类似的，内部 IP 可在 K8S 集群内访问，外部 IP 可在集群外访问。
其次，我们也会关心一下我们的主机名，比如在服务器上执行 hostname 命令，便可得到主机名。K8S 集群中，每个 Node 的主机名也会被记录下来。当然，我们可以通过给 Kubelet 传递一个 --hostname-override 的参数来覆盖默认的主机名。 (Kubelet 是什么，我们后面会解释)
信息 再之后，我们需要看下服务器的基本信息，比如看看系统版本信息， cat /etc/issue 或者 cat /etc/os-release 等方法均可查看。对于 K8S 集群会将每个 Node 的这些基础信息都记录下来。</description>
    </item>
    
    <item>
      <title>01 开篇： Kubernetes 是什么以及为什么需要它</title>
      <link>http://yipsen.github.io/documents/columns/kubernetes/kubernetes%E4%BB%8E%E4%B8%8A%E6%89%8B%E5%88%B0%E5%AE%9E%E8%B7%B5/01-%E5%BC%80%E7%AF%87-kubernetes-%E6%98%AF%E4%BB%80%E4%B9%88%E4%BB%A5%E5%8F%8A%E4%B8%BA%E4%BB%80%E4%B9%88%E9%9C%80%E8%A6%81%E5%AE%83/</link>
      <pubDate>Wed, 22 Dec 2021 01:47:27 +0800</pubDate>
      
      <guid>http://yipsen.github.io/documents/columns/kubernetes/kubernetes%E4%BB%8E%E4%B8%8A%E6%89%8B%E5%88%B0%E5%AE%9E%E8%B7%B5/01-%E5%BC%80%E7%AF%87-kubernetes-%E6%98%AF%E4%BB%80%E4%B9%88%E4%BB%A5%E5%8F%8A%E4%B8%BA%E4%BB%80%E4%B9%88%E9%9C%80%E8%A6%81%E5%AE%83/</guid>
      <description>Kubernetes 是一个可扩展的，用于容器化应用程序编排，管理的平台。由 Google 于 2014 年基于其大规模生产实践经验而开源出来的。Kubernetes 目前在容器编排领域已经成为事实上的标准，社区也非常活跃。
Kubernetes 在国内外都已经得到广泛的应用，无论是Google, Amazon, GitHub 等还是国内的阿里，腾讯，百度，华为，京东或其他中小公司等也都已全力推进 Kubernetes 在生产中的使用。
现在无论是运维，后端，DBA，亦或者是前端，机器学习工程师等都需要在工作中或多或少的用到 Docker， 而在生产中大量使用的话 Kubernetes 也将会成为趋势，所以了解或掌握 Kubernetes 也成为了工程师必不可少的技能之一。
Kubernetes 是什么? 当提到 Kubernetes 的时候，大多数人的可能会想到它可以容器编排，想到它是 PaaS (Platform as a Service) 系统，但其实不然，Kubernetes 并不是 PasS 系统，因为它工作在容器层而不是硬件层，它只不过提供了一些与 PasS 类似或者共同的功能，类似部署，扩容，监控，负载均衡，日志记录等。然而它并不是个完全一体化的平台，这些功能基本都是可选可配置的。
Kubernetes 可支持公有云，私有云及混合云等，具备良好的可移植性。我们可直接使用它或在其之上构建自己的容器/云平台，以达到快速部署，快速扩展，及优化资源使用等。
它致力于提供通用接口类似 CNI( Container Network Interface ), CSI（Container Storage Interface）, CRI（Container Runtime Interface）等规范，以便有更多可能, 让更多的厂商共同加入其生态体系内。它的目标是希望在以后，任何团队都可以在不修改 Kubernetes 核心代码的前提下，可以方便的扩展和构建符合自己需求的平台。
为什么需要 Kubernetes 我们回到实际的工作环境中。
 如果你是个前端，你是否遇到过 npm 依赖安装极慢，或是 node sass 安装不了或者版本不对的情况？ 如果你是个后端，是否遇到过服务器与本地环境不一致的情况，导致部分功能出现非预期的情况？ 如果你是个运维，是否遇到过频繁部署环境，但中间可能出现各种安装不了或者版本不对的问题？  目前来看，对于这些问题，最好的解决方案便是标准化，容器化，现在用到最多的也就是 Docker。 Docker 通过 Dockerfile 来对环境进行描述，通过镜像进行交付，使用时不再需要关注环境不一致相关的问题。</description>
    </item>
    
  </channel>
</rss>
