<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="utf-8">
<meta name="X-UA-Compatible" , content="IE=edge,chrome=1">
<meta name="viewport" content="width=device-width,initial-scale=1.0">

<link rel="icon" type="image/png" href="/%20/favicon.png">

<title>14 庖丁解牛：controller-manager | Yipsen Ye</title>
<meta name="description" content="整体概览 &#43;----------------------------------------------------------&#43; | Master | | &#43;-------------------------&#43; | | &#43;-------&amp;gt;| API Server |&amp;lt;--------&#43; | | | | | | | | v &#43;-------------------------&#43; v | | &#43;----------------&#43; ^ &#43;--------------------&#43; | | | | | | | | | | Scheduler | | | Controller Manager | | | | | | | | | | &#43;----------------&#43; v &#43;--------------------&#43; | | &#43;------------------------------------------------------&#43; | | | | | | | Cluster state store | | | | | | | &#43;------------------------------------------------------&#43; | &#43;----------------------------------------------------------&#43; 在第 3 节《宏观认识：整体架构》 中，我们也认识到了 Controller Manager 的存在，知道了 Master 是 K8S 是集群的大脑，而它则是 Master 中最繁忙的部分。为什么这么说？本节我们一同来看看它为何如此繁忙。">
<meta name="author" content="">

<link rel="stylesheet" type="text/css" href="/styles/main.css">

</head>

<body id="page">
    <header>
        <div class="logo-link" style="width: 60px; color: #CA6924;">
    <a href="http://yipsen.github.io/">
        <img style="margin-bottom: -1rem;" src="/images/deer.svg" alt="logo">
        <span style="font-size: 0.5rem; color: #CA6924;">Yipsen Ye</span>
    </a>
</div>
<nav class="navbar justify-content-end">
    <ul class="nav-list">
        
        
        <li class="nav-item">
            <a class="nav-link"
                href="http://yipsen.github.io/posts/">札记</a>
        </li>
        
        <li class="nav-item">
            <a class="nav-link"
                href="http://yipsen.github.io/collections/">附表</a>
        </li>
        
        <li class="nav-item">
            <a class="nav-link"
                href="http://yipsen.github.io/about/">关于</a>
        </li>
        
    </ul>
</nav>
    </header>
    <main id="content">
        
<div class="container"><aside>
    <div>
        
        
            
            
            <div class="post-category-icon"></div>
            <a href="/categories/kubernetes%e4%bb%8e%e4%b8%8a%e6%89%8b%e5%88%b0%e5%ae%9e%e8%b7%b5/">Kubernetes从上手到实践</a>
            <ul>
                
                
                    <li><a href="http://yipsen.github.io/documents/columns/kubernetes/kubernetes%E4%BB%8E%E4%B8%8A%E6%89%8B%E5%88%B0%E5%AE%9E%E8%B7%B5/01-%E5%BC%80%E7%AF%87-kubernetes-%E6%98%AF%E4%BB%80%E4%B9%88%E4%BB%A5%E5%8F%8A%E4%B8%BA%E4%BB%80%E4%B9%88%E9%9C%80%E8%A6%81%E5%AE%83/">01 开篇： Kubernetes 是什么以及为什么需要它</a></li>
                
                
                
                    <li><a href="http://yipsen.github.io/documents/columns/kubernetes/kubernetes%E4%BB%8E%E4%B8%8A%E6%89%8B%E5%88%B0%E5%AE%9E%E8%B7%B5/02-%E5%88%9D%E6%AD%A5%E8%AE%A4%E8%AF%86kubernetes-%E5%9F%BA%E7%A1%80%E6%A6%82%E5%BF%B5/">02 初步认识：Kubernetes 基础概念</a></li>
                
                
                
                    <li><a href="http://yipsen.github.io/documents/columns/kubernetes/kubernetes%E4%BB%8E%E4%B8%8A%E6%89%8B%E5%88%B0%E5%AE%9E%E8%B7%B5/03-%E5%AE%8F%E8%A7%82%E8%AE%A4%E8%AF%86%E6%95%B4%E4%BD%93%E6%9E%B6%E6%9E%84/">03 宏观认识：整体架构</a></li>
                
                
                
                    <li><a href="http://yipsen.github.io/documents/columns/kubernetes/kubernetes%E4%BB%8E%E4%B8%8A%E6%89%8B%E5%88%B0%E5%AE%9E%E8%B7%B5/04-%E6%90%AD%E5%BB%BA-kubernetes-%E9%9B%86%E7%BE%A4-%E6%9C%AC%E5%9C%B0%E5%BF%AB%E9%80%9F%E6%90%AD%E5%BB%BA/">04 搭建 Kubernetes 集群 - 本地快速搭建</a></li>
                
                
                
                    <li><a href="http://yipsen.github.io/documents/columns/kubernetes/kubernetes%E4%BB%8E%E4%B8%8A%E6%89%8B%E5%88%B0%E5%AE%9E%E8%B7%B5/05-%E5%8A%A8%E6%89%8B%E5%AE%9E%E8%B7%B5%E6%90%AD%E5%BB%BA%E4%B8%80%E4%B8%AA-kubernetes-%E9%9B%86%E7%BE%A4-%E7%94%9F%E4%BA%A7%E5%8F%AF%E7%94%A8/">05 动手实践：搭建一个 Kubernetes 集群 - 生产可用</a></li>
                
                
                
                    <li><a href="http://yipsen.github.io/documents/columns/kubernetes/kubernetes%E4%BB%8E%E4%B8%8A%E6%89%8B%E5%88%B0%E5%AE%9E%E8%B7%B5/06-%E9%9B%86%E7%BE%A4%E7%AE%A1%E7%90%86%E5%88%9D%E8%AF%86-kubectl/">06 集群管理：初识 kubectl</a></li>
                
                
                
                    <li><a href="http://yipsen.github.io/documents/columns/kubernetes/kubernetes%E4%BB%8E%E4%B8%8A%E6%89%8B%E5%88%B0%E5%AE%9E%E8%B7%B5/07-%E9%9B%86%E7%BE%A4%E7%AE%A1%E7%90%86%E4%BB%A5-redis-%E4%B8%BA%E4%BE%8B-%E9%83%A8%E7%BD%B2%E5%8F%8A%E8%AE%BF%E9%97%AE/">07 集群管理：以 Redis 为例-部署及访问</a></li>
                
                
                
                    <li><a href="http://yipsen.github.io/documents/columns/kubernetes/kubernetes%E4%BB%8E%E4%B8%8A%E6%89%8B%E5%88%B0%E5%AE%9E%E8%B7%B5/08-%E5%AE%89%E5%85%A8%E9%87%8D%E7%82%B9-%E8%AE%A4%E8%AF%81%E5%92%8C%E6%8E%88%E6%9D%83/">08 安全重点 认证和授权</a></li>
                
                
                
                    <li><a href="http://yipsen.github.io/documents/columns/kubernetes/kubernetes%E4%BB%8E%E4%B8%8A%E6%89%8B%E5%88%B0%E5%AE%9E%E8%B7%B5/09-%E5%BA%94%E7%94%A8%E5%8F%91%E5%B8%83%E9%83%A8%E7%BD%B2%E5%AE%9E%E9%99%85%E9%A1%B9%E7%9B%AE/">09 应用发布：部署实际项目</a></li>
                
                
                
                    <li><a href="http://yipsen.github.io/documents/columns/kubernetes/kubernetes%E4%BB%8E%E4%B8%8A%E6%89%8B%E5%88%B0%E5%AE%9E%E8%B7%B5/10-%E5%BA%94%E7%94%A8%E7%AE%A1%E7%90%86%E5%88%9D%E8%AF%86-helm/">10 应用管理：初识 Helm</a></li>
                
                
                
                    <li><a href="http://yipsen.github.io/documents/columns/kubernetes/kubernetes%E4%BB%8E%E4%B8%8A%E6%89%8B%E5%88%B0%E5%AE%9E%E8%B7%B5/11-%E9%83%A8%E7%BD%B2%E5%AE%9E%E8%B7%B5%E4%BB%A5-helm-%E9%83%A8%E7%BD%B2%E9%A1%B9%E7%9B%AE/">11 部署实践：以 Helm 部署项目</a></li>
                
                
                
                    <li><a href="http://yipsen.github.io/documents/columns/kubernetes/kubernetes%E4%BB%8E%E4%B8%8A%E6%89%8B%E5%88%B0%E5%AE%9E%E8%B7%B5/12-%E5%BA%96%E4%B8%81%E8%A7%A3%E7%89%9Bkube-apiserver/">12 庖丁解牛：kube-apiserver</a></li>
                
                
                
                    <li><a href="http://yipsen.github.io/documents/columns/kubernetes/kubernetes%E4%BB%8E%E4%B8%8A%E6%89%8B%E5%88%B0%E5%AE%9E%E8%B7%B5/13-%E5%BA%96%E4%B8%81%E8%A7%A3%E7%89%9Betcd/">13 庖丁解牛：etcd</a></li>
                
                
                
                    <li>14 庖丁解牛：controller-manager</li>
                
                
                
                    <li><a href="http://yipsen.github.io/documents/columns/kubernetes/kubernetes%E4%BB%8E%E4%B8%8A%E6%89%8B%E5%88%B0%E5%AE%9E%E8%B7%B5/15-%E5%BA%96%E4%B8%81%E8%A7%A3%E7%89%9Bkube-scheduler/">15 庖丁解牛：kube-scheduler</a></li>
                
                
                
                    <li><a href="http://yipsen.github.io/documents/columns/kubernetes/kubernetes%E4%BB%8E%E4%B8%8A%E6%89%8B%E5%88%B0%E5%AE%9E%E8%B7%B5/16-%E5%BA%96%E4%B8%81%E8%A7%A3%E7%89%9Bkubelet/">16 庖丁解牛：kubelet</a></li>
                
                
                
                    <li><a href="http://yipsen.github.io/documents/columns/kubernetes/kubernetes%E4%BB%8E%E4%B8%8A%E6%89%8B%E5%88%B0%E5%AE%9E%E8%B7%B5/17-%E5%BA%96%E4%B8%81%E8%A7%A3%E7%89%9Bkube-proxy/">17 庖丁解牛：kube-proxy</a></li>
                
                
                
                    <li><a href="http://yipsen.github.io/documents/columns/kubernetes/kubernetes%E4%BB%8E%E4%B8%8A%E6%89%8B%E5%88%B0%E5%AE%9E%E8%B7%B5/18-%E5%BA%96%E4%B8%81%E8%A7%A3%E7%89%9Bcontainer-runtime-docker/">18 庖丁解牛：Container Runtime （Docker）</a></li>
                
                
                
                    <li><a href="http://yipsen.github.io/documents/columns/kubernetes/kubernetes%E4%BB%8E%E4%B8%8A%E6%89%8B%E5%88%B0%E5%AE%9E%E8%B7%B5/19-troubleshoot/">19 Troubleshoot</a></li>
                
                
                
                    <li><a href="http://yipsen.github.io/documents/columns/kubernetes/kubernetes%E4%BB%8E%E4%B8%8A%E6%89%8B%E5%88%B0%E5%AE%9E%E8%B7%B5/20-%E6%89%A9%E5%B1%95%E5%A2%9E%E5%BC%BAdashboard/">20 扩展增强：Dashboard</a></li>
                
                
                
                    <li><a href="http://yipsen.github.io/documents/columns/kubernetes/kubernetes%E4%BB%8E%E4%B8%8A%E6%89%8B%E5%88%B0%E5%AE%9E%E8%B7%B5/21-%E6%89%A9%E5%B1%95%E5%A2%9E%E5%BC%BAcoredns/">21 扩展增强：CoreDNS</a></li>
                
                
                
                    <li><a href="http://yipsen.github.io/documents/columns/kubernetes/kubernetes%E4%BB%8E%E4%B8%8A%E6%89%8B%E5%88%B0%E5%AE%9E%E8%B7%B5/22-%E6%9C%8D%E5%8A%A1%E5%A2%9E%E5%BC%BAingress/">22 服务增强：Ingress</a></li>
                
                
                
                    <li><a href="http://yipsen.github.io/documents/columns/kubernetes/kubernetes%E4%BB%8E%E4%B8%8A%E6%89%8B%E5%88%B0%E5%AE%9E%E8%B7%B5/23-%E7%9B%91%E6%8E%A7%E5%AE%9E%E8%B7%B5%E5%AF%B9-k8s-%E9%9B%86%E7%BE%A4%E8%BF%9B%E8%A1%8C%E7%9B%91%E6%8E%A7/">23 监控实践：对 K8S 集群进行监控</a></li>
                
                
                
                    <li><a href="http://yipsen.github.io/documents/columns/kubernetes/kubernetes%E4%BB%8E%E4%B8%8A%E6%89%8B%E5%88%B0%E5%AE%9E%E8%B7%B5/24-%E6%80%BB%E7%BB%93/">24 总结</a></li>
                
                
            </ul>
        
    </div>
</aside>

<style>
    aside {
        position: fixed;
        left: 20px;
        background: #ffdee3;
        border-radius: 6px;
        padding: 20px;
        padding-top: 60px;
        box-shadow: 5px 10px #888;
        list-style-type: none;
        display: none;
    }

    .post-category-icon {
        position: absolute;
        width: 80px;
        height: 80px;
        top: -40px;
        left: 50%;
        margin-left: -40px;
        text-align: center;
        font-size: 36px;
        content: 'J';
        background: #ffdee3;
        border-radius: 100%;
        border: 10px solid #fff;
        box-shadow: 5px 10px #888;
    }
</style><article class="post-block">
        <h1 class="post-title">14 庖丁解牛：controller-manager</h1>
        <div class="post-info">
            <div> 
                <span class="post-date">📅&nbsp;2021-12-22 01:47:40</span>🏷️&nbsp;<a class="post-tag" href="/tags/kubernetes/">kubernetes</a></div><div class="post-category">
                
                📚&nbsp;<a href="/categories/kubernetes%E4%BB%8E%E4%B8%8A%E6%89%8B%E5%88%B0%E5%AE%9E%E8%B7%B5/">Kubernetes从上手到实践</a>
                
            </div></div>
        <div class="post-content">
            <h2 id="整体概览">整体概览</h2>
<pre tabindex="0"><code>+----------------------------------------------------------+          
| Master                                                   |          
|              +-------------------------+                 |          
|     +-------&gt;|        API Server       |&lt;--------+       |          
|     |        |                         |         |       |          
|     v        +-------------------------+         v       |          
|   +----------------+     ^      +--------------------+   |          
|   |                |     |      |                    |   |          
|   |   Scheduler    |     |      | Controller Manager |   |          
|   |                |     |      |                    |   |          
|   +----------------+     v      +--------------------+   |          
| +------------------------------------------------------+ |          
| |                                                      | |          
| |                Cluster state store                   | |          
| |                                                      | |          
| +------------------------------------------------------+ |          
+----------------------------------------------------------+          
</code></pre><p>在第 3 节《宏观认识：整体架构》 中，我们也认识到了 <code>Controller Manager</code> 的存在，知道了 Master 是 K8S 是集群的大脑，而它则是 Master 中最繁忙的部分。为什么这么说？本节我们一同来看看它为何如此繁忙。</p>
<p><strong>注意：Controller Manager 实际由 kube-controller-manager 和 cloud-controller-manager 两部分组成，cloud-controller-manager 则是为各家云厂商提供了一个抽象的封装，便于让各厂商使用各自的 provide。本文只讨论 kube-controller-manager，为了避免混淆，下文统一使用 kube-controller-manager。</strong></p>
<h2 id="kube-controller-manager-是什么"><code>kube-controller-manager</code> 是什么</h2>
<p>一句话来讲 <code>kube-controller-manager</code> 是一个嵌入了 K8S 核心控制循环的守护进程。</p>
<p>这里的重点是</p>
<ul>
<li>嵌入：它已经内置了相关逻辑，可独立进行部署。我们在第 5 节下载 K8S 服务端二进制文件解压后，便可以看到 <code>kube-controller-manager</code> 的可执行文件，不过我们使用的是 <code>kubeadm</code> 进行的部署，它会默认使用 <code>k8s.gcr.io/kube-controller-manager</code> 的镜像。我们直接来看下实际情况：</li>
</ul>
<pre tabindex="0"><code>master $ kubectl -n kube-system describe pods -l component=kube-controller-manager
Name:               kube-controller-manager-master
Namespace:          kube-system
Priority:           2000000000
PriorityClassName:  system-cluster-critical
Node:               master/172.17.0.35
Start Time:         Mon, 10 Dec 2018 07:14:21 +0000
Labels:             component=kube-controller-manager
                    tier=control-plane
Annotations:        kubernetes.io/config.hash=c7ed7a8fa5c430410e84970f8ee7e067
                    kubernetes.io/config.mirror=c7ed7a8fa5c430410e84970f8ee7e067
                    kubernetes.io/config.seen=2018-12-10T07:14:21.685626322Z
                    kubernetes.io/config.source=file
                    scheduler.alpha.kubernetes.io/critical-pod=
Status:             Running
IP:                 172.17.0.35
Containers:
  kube-controller-manager:
    Container ID:  docker://0653e71ae4287608726490b724c3d064d5f1556dd89b7d3c618e97f0e7f2a533
    Image:         k8s.gcr.io/kube-controller-manager-amd64:v1.11.3
    Image ID:      docker-pullable://k8s.gcr.io/kube-controller-manager-amd64@sha256:a6d115bb1c0116036ac6e6e4d504665bc48879c421a450566c38b3b726f0a123
    Port:          &lt;none&gt;
    Host Port:     &lt;none&gt;
    Command:
      kube-controller-manager
      --address=127.0.0.1
      --cluster-signing-cert-file=/etc/kubernetes/pki/ca.crt
      --cluster-signing-key-file=/etc/kubernetes/pki/ca.key
      --controllers=*,bootstrapsigner,tokencleaner
      --kubeconfig=/etc/kubernetes/controller-manager.conf
      --leader-elect=true
      --root-ca-file=/etc/kubernetes/pki/ca.crt
      --service-account-private-key-file=/etc/kubernetes/pki/sa.key
      --use-service-account-credentials=true
    State:          Running
      Started:      Mon, 10 Dec 2018 07:14:24 +0000
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        200m
    Liveness:     http-get http://127.0.0.1:10252/healthz delay=15s timeout=15s period=10s #success=1 #failure=8
    Environment:  &lt;none&gt;
    Mounts:
      /etc/ca-certificates from etc-ca-certificates (ro)
      /etc/kubernetes/controller-manager.conf from kubeconfig (ro)
      /etc/kubernetes/pki from k8s-certs (ro)
      /etc/ssl/certs from ca-certs (ro)
      /usr/libexec/kubernetes/kubelet-plugins/volume/exec from flexvolume-dir (rw)
      /usr/local/share/ca-certificates from usr-local-share-ca-certificates (ro)
      /usr/share/ca-certificates from usr-share-ca-certificates (ro)
Conditions:
  Type              Status
  Initialized       True
  Ready             True
  ContainersReady   True
  PodScheduled      True
Volumes:
  usr-share-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /usr/share/ca-certificates
    HostPathType:  DirectoryOrCreate
  usr-local-share-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /usr/local/share/ca-certificates
    HostPathType:  DirectoryOrCreate
  etc-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/ca-certificates
    HostPathType:  DirectoryOrCreate
  k8s-certs:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/kubernetes/pki
    HostPathType:  DirectoryOrCreate
  ca-certs:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/ssl/certs
    HostPathType:  DirectoryOrCreate
  kubeconfig:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/kubernetes/controller-manager.conf
    HostPathType:  FileOrCreate
  flexvolume-dir:
    Type:          HostPath (bare host directory volume)
    Path:          /usr/libexec/kubernetes/kubelet-plugins/volume/exec
    HostPathType:  DirectoryOrCreate
QoS Class:         Burstable
Node-Selectors:    &lt;none&gt;
Tolerations:       :NoExecute
Events:            &lt;none&gt;
master
</code></pre><p>这是使用 <code>kubeadm</code> 搭建的集群中的 <code>kube-controller-manager</code> 的 <code>Pod</code>，首先可以看到它所使用的镜像，其次可以看到它使用的一系列参数，最后它在 <code>10252</code> 端口提供了健康检查的接口。稍后我们再展开。</p>
<ul>
<li>控制循环：这里拆解为两部分： <strong>控制</strong> 和 <strong>循环</strong> ，它所控制的是集群的状态；至于循环它当然是会有个循环间隔的，这里有个参数可以进行控制。</li>
<li>守护进程：这个就不单独展开了。</li>
</ul>
<h2 id="kube-controller-manager-有什么作用"><code>kube-controller-manager</code> 有什么作用</h2>
<p>前面已经说了它一个很关键的点 “控制”：它通过 <code>kube-apiserver</code> 提供的信息持续的监控集群状态，并尝试将集群调整至预期的状态。由于访问 <code>kube-apiserver</code> 也需要通过认证，授权等过程，所以可以看到上面启动 <code>kube-controller-manager</code> 时提供了一系列的参数。</p>
<p>比如，当我们创建了一个 <code>Deployment</code>，默认副本数为 1 ，当我们把 <code>Pod</code> 删除后，<code>kube-controller-manager</code> 会按照原先的预期，重新创建一个 <code>Pod</code> 。下面举个例子：</p>
<pre tabindex="0"><code>master $ kubectl run redis --image='redis'
deployment.apps/redis created
master $ kubectl get all
NAME                        READY     STATUS    RESTARTS   AGE
pod/redis-bb7894d65-w2rsp   1/1       Running   0          3m

NAME                 TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE
service/kubernetes   ClusterIP   10.96.0.1    &lt;none&gt;        443/TCP   18m

NAME                    DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/redis   1         1         1            1           3m

NAME                              DESIRED   CURRENT   READY     AGE
replicaset.apps/redis-bb7894d65   1         1         1         3m
master $ kubectl delete pod/redis-bb7894d65-w2rsp
pod &quot;redis-bb7894d65-w2rsp&quot; deleted
master $ kubectl get all  # 可以看到已经重新运行了一个 Pod
NAME                        READY     STATUS    RESTARTS   AGE
pod/redis-bb7894d65-62ftk   1/1       Running   0          16s

NAME                 TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE
service/kubernetes   ClusterIP   10.96.0.1    &lt;none&gt;        443/TCP   19m

NAME                    DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/redis   1         1         1            1           4m

NAME                              DESIRED   CURRENT   READY     AGE
replicaset.apps/redis-bb7894d65   1         1         1         4m
</code></pre><p>我们来看下 <code>kube-controller-manager</code> 的日志：</p>
<pre tabindex="0"><code>master $ kubectl -n kube-system logs -l component=kube-controller-manager --tail=5
I1210 09:30:17.125377       1 node_lifecycle_controller.go:945] Controller detected that all Nodes are not-Ready. Entering master disruption mode.
I1210 09:31:07.140539       1 node_lifecycle_controller.go:972] Controller detected that some Nodes are Ready. Exiting master disruption mode.
I1210 09:43:30.377649       1 event.go:221] Event(v1.ObjectReference{Kind:&quot;Deployment&quot;, Namespace:&quot;default&quot;, Name:&quot;redis&quot;, UID:&quot;0d1cb2d7-fc60-11e8-a361-0242ac110074&quot;, APIVersion:&quot;apps/v1&quot;, ResourceVersion:&quot;1494&quot;, FieldPath:&quot;&quot;}): type: 'Normal' reason: 'ScalingReplicaSet' Scaled up replica setredis-bb7894d65 to 1
I1210 09:43:30.835149       1 event.go:221] Event(v1.ObjectReference{Kind:&quot;ReplicaSet&quot;, Namespace:&quot;default&quot;, Name:&quot;redis-bb7894d65&quot;, UID:&quot;0d344d15-fc60-11e8-a361-0242ac110074&quot;, APIVersion:&quot;apps/v1&quot;, ResourceVersion:&quot;1495&quot;, FieldPath:&quot;&quot;}): type: 'Normal' reason: 'SuccessfulCreate' Created pod:redis-bb7894d65-w2rsp
I1210 09:47:41.658781       1 event.go:221] Event(v1.ObjectReference{Kind:&quot;ReplicaSet&quot;, Namespace:&quot;default&quot;, Name:&quot;redis-bb7894d65&quot;, UID:&quot;0d344d15-fc60-11e8-a361-0242ac110074&quot;, APIVersion:&quot;apps/v1&quot;, ResourceVersion:&quot;1558&quot;, FieldPath:&quot;&quot;}): type: 'Normal' reason: 'SuccessfulCreate' Created pod:redis-bb7894d65-62ftk
</code></pre><p>可以看到它先观察到有 <code>Deployment</code> 的事件，然后 <code>ScalingReplicaSet</code> 进而创建了对应的 <code>Pod</code>。 而当我们删掉正在运行的 <code>Pod</code> 后，它便会重新创建 <code>Pod</code> 使集群状态符合原先的预期状态。</p>
<p>同时，注意 <code>Pod</code> 的名字已经发生了变化。</p>
<h2 id="kube-controller-manager-是如何工作的"><code>kube-controller-manager</code> 是如何工作的</h2>
<p>在 <code>cmd/kube-controller-manager/app/controllermanager.go</code> 中列出了大多数的 <code>controllermanager</code>，他们对 <code>controllermanager</code> 函数的实际调用都在 <code>cmd/kube-controller-manager/app/core.go</code> 中，我们以 <code>PodGC</code> 为例：</p>
<pre tabindex="0"><code>func startPodGCController(ctx ControllerContext) (bool, error) {
go podgc.NewPodGC(
ctx.ClientBuilder.ClientOrDie(&quot;pod-garbage-collector&quot;),
ctx.InformerFactory.Core().V1().Pods(),
int(ctx.ComponentConfig.PodGCController.TerminatedPodGCThreshold),
).Run(ctx.Stop)
return true, nil
}
</code></pre><p>在前两节中我们已经对 <code>kube-apiserver</code> 和 <code>etcd</code> 有了一些基本的认识，这里它主要会去 watch 相关的资源，但是出于性能上的考虑，也不能过于频繁的去请求 <code>kube-apiserver</code> 或者永久 watch ，所以在实现上借助了 <a href="https://github.com/kubernetes/client-go">client-go</a> 的 <code>informer</code> 包，相当于是实现了一个本地的二级缓存。这里不做过多展开。</p>
<p>它最终会调用 <code>PodGC</code> 的具体实现，位置在 <code>pkg/controller/podgc/gc_controller.go</code> 中：</p>
<pre tabindex="0"><code>func NewPodGC(kubeClient clientset.Interface, podInformer coreinformers.PodInformer, terminatedPodThreshold int) *PodGCController {
if kubeClient != nil &amp;&amp; kubeClient.CoreV1().RESTClient().GetRateLimiter() != nil {
metrics.RegisterMetricAndTrackRateLimiterUsage(&quot;gc_controller&quot;, kubeClient.CoreV1().RESTClient().GetRateLimiter())
}
gcc := &amp;PodGCController{
kubeClient:             kubeClient,
terminatedPodThreshold: terminatedPodThreshold,
deletePod: func(namespace, name string) error {
glog.Infof(&quot;PodGC is force deleting Pod: %v:%v&quot;, namespace, name)
return kubeClient.CoreV1().Pods(namespace).Delete(name, metav1.NewDeleteOptions(0))
},
}

gcc.podLister = podInformer.Lister()
gcc.podListerSynced = podInformer.Informer().HasSynced

return gcc
}
</code></pre><p>代码也比较直观，不过这里可以看到有一个注册 <code>metrics</code> 的过程，实际上 <code>kube-controller-manager</code> 在前面的 <code>10252</code> 端口上不仅暴露出来了一个 <code>/healthz</code> 接口，还暴露出了一个 <code>/metrics</code> 的接口，可用于进行监控之类的。</p>
<pre tabindex="0"><code>master $ kubectl -n kube-system get pod -l component=kube-controller-manager
NAME                             READY     STATUS    RESTARTS   AGE
kube-controller-manager-master   1/1       Running   1          2m
master $ kubectl -n kube-system exec -it kube-controller-manager-master sh
/ # wget -qO- http://127.0.0.1:10252/metrics|grep gc_controller
# HELP gc_controller_rate_limiter_use A metric measuring the saturation of the rate limiter for gc_controller
# TYPE gc_controller_rate_limiter_use gauge
gc_controller_rate_limiter_use 0
</code></pre><h2 id="总结">总结</h2>
<p>在本节中，我们介绍了 <code>kube-controller-manager</code> 以及它在 K8S 中主要是将集群调节至预期的状态，并提供出了 <code>/metrics</code> 的接口可供监控。</p>
<p><code>kube-controller-manager</code> 中有很多的 controller 大多数是默认开启的，当然也有默认关闭的，比如 <code>bootstrapsigner</code> 和 <code>tokencleaner</code>，在我们启动 <code>kube-controller-manager</code> 的时候，可通过 <code>--controllers</code> 的参数进行控制，就比如上面例子中 <code>--controllers=*,bootstrapsigner,tokencleaner</code> 表示开启所有默认开启的以及 <code>bootstrapsigner</code> 和 <code>tokencleaner</code> 。</p>
<p>下节，我们将学习另一个与资源调度有关的组件 <code>kube-scheduler</code>，了解下它对我们使用集群所带来的意义。</p>

        </div>
    </article>
</div>

    </main>
    <footer>
        <div id="pagination">
            
<div class="paginator">
    <div class="prev">
        
        <a href="/documents/columns/kubernetes/kubernetes%E4%BB%8E%E4%B8%8A%E6%89%8B%E5%88%B0%E5%AE%9E%E8%B7%B5/13-%E5%BA%96%E4%B8%81%E8%A7%A3%E7%89%9Betcd/"><span>13 庖丁解牛：etcd</span></a>
    </div>
    <div class="next">
        
        <a href="/documents/columns/kubernetes/kubernetes%E4%BB%8E%E4%B8%8A%E6%89%8B%E5%88%B0%E5%AE%9E%E8%B7%B5/15-%E5%BA%96%E4%B8%81%E8%A7%A3%E7%89%9Bkube-scheduler/"><span>15 庖丁解牛：kube-scheduler</span></a>
    </div>
</div>

        </div>
        <div class="toggler" onclick="toggleLight(this);">💡</div>
        <div id="copyright" style="display: none;">
    
    
    <p>&copy; 28280 <a href="/"></a>, powered by Hugo and Qiao</p>
</div>
    </footer>
    
<script>
    function toggleLight() {
        document.getElementById('page').classList.toggle('night');
    }
</script>
</body>

</html>