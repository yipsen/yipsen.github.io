<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="utf-8">
<meta name="X-UA-Compatible" , content="IE=edge,chrome=1">
<meta name="viewport" content="width=device-width,initial-scale=1.0">

<link rel="icon" type="image/png" href="/%20/favicon.png">

<title>17 庖丁解牛：kube-proxy | Yipsen Ye</title>
<meta name="description" content="整体概览 在第 3 节中，我们了解到 kube-proxy 的存在，而在第 7 中，我们学习到了如何将运行于 K8S 中的服务以 Service 的方式暴露出来，以供访问。
本节，我们来介绍下 kube-proxy 了解下它是如何支撑起这种类似服务发现和代理相关功能的。
kube-proxy 是什么 kube-proxy 是 K8S 运行于每个 Node 上的网络代理组件，提供了 TCP 和 UDP 的连接转发支持。
我们已经知道，当 Pod 在创建和销毁的过程中，IP 可能会发生变化，而这就容易造成对其有依赖的服务的异常，所以通常情况下，我们都会使用 Service 将后端 Pod 暴露出来，而 Service 则较为稳定。
还是以我们之前的 SayThx 项目为例，但我们只部署其中没有任何依赖的后端资源 Redis 。
master $ git clone https://github.com/tao12345666333/saythx.gitCloning into &#39;saythx&#39;...remote: Enumerating objects: 110, done.remote: Counting objects: 100% (110/110), done.remote: Compressing objects: 100% (82/82), done.remote: Total 110 (delta 27), reused 102 (delta 20), pack-reused 0Receiving objects: 100% (110/110), 119.">
<meta name="author" content="">

<link rel="stylesheet" type="text/css" href="/styles/main.css">

</head>

<body id="page">
    <header>
        <div class="logo-link" style="width: 60px; color: #CA6924;">
    <a href="http://yipsen.github.io/">
        <img style="margin-bottom: -1rem;" src="/images/deer.svg" alt="logo">
        <span style="font-size: 0.5rem; color: #CA6924;">Yipsen Ye</span>
    </a>
</div>
<nav class="navbar justify-content-end">
    <ul class="nav-list">
        
        
        <li class="nav-item">
            <a class="nav-link"
                href="http://yipsen.github.io/posts/">札记</a>
        </li>
        
        <li class="nav-item">
            <a class="nav-link"
                href="http://yipsen.github.io/collections/">附表</a>
        </li>
        
        <li class="nav-item">
            <a class="nav-link"
                href="http://yipsen.github.io/about/">关于</a>
        </li>
        
    </ul>
</nav>
    </header>
    <main id="content">
        
<div class="container"><aside>
    <div>
        
        
            
            
            <div class="post-category-icon"></div>
            <a href="/categories/kubernetes%e4%bb%8e%e4%b8%8a%e6%89%8b%e5%88%b0%e5%ae%9e%e8%b7%b5/">Kubernetes从上手到实践</a>
            <ul>
                
                
                    <li><a href="http://yipsen.github.io/documents/columns/kubernetes/kubernetes%E4%BB%8E%E4%B8%8A%E6%89%8B%E5%88%B0%E5%AE%9E%E8%B7%B5/01-%E5%BC%80%E7%AF%87-kubernetes-%E6%98%AF%E4%BB%80%E4%B9%88%E4%BB%A5%E5%8F%8A%E4%B8%BA%E4%BB%80%E4%B9%88%E9%9C%80%E8%A6%81%E5%AE%83/">01 开篇： Kubernetes 是什么以及为什么需要它</a></li>
                
                
                
                    <li><a href="http://yipsen.github.io/documents/columns/kubernetes/kubernetes%E4%BB%8E%E4%B8%8A%E6%89%8B%E5%88%B0%E5%AE%9E%E8%B7%B5/02-%E5%88%9D%E6%AD%A5%E8%AE%A4%E8%AF%86kubernetes-%E5%9F%BA%E7%A1%80%E6%A6%82%E5%BF%B5/">02 初步认识：Kubernetes 基础概念</a></li>
                
                
                
                    <li><a href="http://yipsen.github.io/documents/columns/kubernetes/kubernetes%E4%BB%8E%E4%B8%8A%E6%89%8B%E5%88%B0%E5%AE%9E%E8%B7%B5/03-%E5%AE%8F%E8%A7%82%E8%AE%A4%E8%AF%86%E6%95%B4%E4%BD%93%E6%9E%B6%E6%9E%84/">03 宏观认识：整体架构</a></li>
                
                
                
                    <li><a href="http://yipsen.github.io/documents/columns/kubernetes/kubernetes%E4%BB%8E%E4%B8%8A%E6%89%8B%E5%88%B0%E5%AE%9E%E8%B7%B5/04-%E6%90%AD%E5%BB%BA-kubernetes-%E9%9B%86%E7%BE%A4-%E6%9C%AC%E5%9C%B0%E5%BF%AB%E9%80%9F%E6%90%AD%E5%BB%BA/">04 搭建 Kubernetes 集群 - 本地快速搭建</a></li>
                
                
                
                    <li><a href="http://yipsen.github.io/documents/columns/kubernetes/kubernetes%E4%BB%8E%E4%B8%8A%E6%89%8B%E5%88%B0%E5%AE%9E%E8%B7%B5/05-%E5%8A%A8%E6%89%8B%E5%AE%9E%E8%B7%B5%E6%90%AD%E5%BB%BA%E4%B8%80%E4%B8%AA-kubernetes-%E9%9B%86%E7%BE%A4-%E7%94%9F%E4%BA%A7%E5%8F%AF%E7%94%A8/">05 动手实践：搭建一个 Kubernetes 集群 - 生产可用</a></li>
                
                
                
                    <li><a href="http://yipsen.github.io/documents/columns/kubernetes/kubernetes%E4%BB%8E%E4%B8%8A%E6%89%8B%E5%88%B0%E5%AE%9E%E8%B7%B5/06-%E9%9B%86%E7%BE%A4%E7%AE%A1%E7%90%86%E5%88%9D%E8%AF%86-kubectl/">06 集群管理：初识 kubectl</a></li>
                
                
                
                    <li><a href="http://yipsen.github.io/documents/columns/kubernetes/kubernetes%E4%BB%8E%E4%B8%8A%E6%89%8B%E5%88%B0%E5%AE%9E%E8%B7%B5/07-%E9%9B%86%E7%BE%A4%E7%AE%A1%E7%90%86%E4%BB%A5-redis-%E4%B8%BA%E4%BE%8B-%E9%83%A8%E7%BD%B2%E5%8F%8A%E8%AE%BF%E9%97%AE/">07 集群管理：以 Redis 为例-部署及访问</a></li>
                
                
                
                    <li><a href="http://yipsen.github.io/documents/columns/kubernetes/kubernetes%E4%BB%8E%E4%B8%8A%E6%89%8B%E5%88%B0%E5%AE%9E%E8%B7%B5/08-%E5%AE%89%E5%85%A8%E9%87%8D%E7%82%B9-%E8%AE%A4%E8%AF%81%E5%92%8C%E6%8E%88%E6%9D%83/">08 安全重点 认证和授权</a></li>
                
                
                
                    <li><a href="http://yipsen.github.io/documents/columns/kubernetes/kubernetes%E4%BB%8E%E4%B8%8A%E6%89%8B%E5%88%B0%E5%AE%9E%E8%B7%B5/09-%E5%BA%94%E7%94%A8%E5%8F%91%E5%B8%83%E9%83%A8%E7%BD%B2%E5%AE%9E%E9%99%85%E9%A1%B9%E7%9B%AE/">09 应用发布：部署实际项目</a></li>
                
                
                
                    <li><a href="http://yipsen.github.io/documents/columns/kubernetes/kubernetes%E4%BB%8E%E4%B8%8A%E6%89%8B%E5%88%B0%E5%AE%9E%E8%B7%B5/10-%E5%BA%94%E7%94%A8%E7%AE%A1%E7%90%86%E5%88%9D%E8%AF%86-helm/">10 应用管理：初识 Helm</a></li>
                
                
                
                    <li><a href="http://yipsen.github.io/documents/columns/kubernetes/kubernetes%E4%BB%8E%E4%B8%8A%E6%89%8B%E5%88%B0%E5%AE%9E%E8%B7%B5/11-%E9%83%A8%E7%BD%B2%E5%AE%9E%E8%B7%B5%E4%BB%A5-helm-%E9%83%A8%E7%BD%B2%E9%A1%B9%E7%9B%AE/">11 部署实践：以 Helm 部署项目</a></li>
                
                
                
                    <li><a href="http://yipsen.github.io/documents/columns/kubernetes/kubernetes%E4%BB%8E%E4%B8%8A%E6%89%8B%E5%88%B0%E5%AE%9E%E8%B7%B5/12-%E5%BA%96%E4%B8%81%E8%A7%A3%E7%89%9Bkube-apiserver/">12 庖丁解牛：kube-apiserver</a></li>
                
                
                
                    <li><a href="http://yipsen.github.io/documents/columns/kubernetes/kubernetes%E4%BB%8E%E4%B8%8A%E6%89%8B%E5%88%B0%E5%AE%9E%E8%B7%B5/13-%E5%BA%96%E4%B8%81%E8%A7%A3%E7%89%9Betcd/">13 庖丁解牛：etcd</a></li>
                
                
                
                    <li><a href="http://yipsen.github.io/documents/columns/kubernetes/kubernetes%E4%BB%8E%E4%B8%8A%E6%89%8B%E5%88%B0%E5%AE%9E%E8%B7%B5/14-%E5%BA%96%E4%B8%81%E8%A7%A3%E7%89%9Bcontroller-manager/">14 庖丁解牛：controller-manager</a></li>
                
                
                
                    <li><a href="http://yipsen.github.io/documents/columns/kubernetes/kubernetes%E4%BB%8E%E4%B8%8A%E6%89%8B%E5%88%B0%E5%AE%9E%E8%B7%B5/15-%E5%BA%96%E4%B8%81%E8%A7%A3%E7%89%9Bkube-scheduler/">15 庖丁解牛：kube-scheduler</a></li>
                
                
                
                    <li><a href="http://yipsen.github.io/documents/columns/kubernetes/kubernetes%E4%BB%8E%E4%B8%8A%E6%89%8B%E5%88%B0%E5%AE%9E%E8%B7%B5/16-%E5%BA%96%E4%B8%81%E8%A7%A3%E7%89%9Bkubelet/">16 庖丁解牛：kubelet</a></li>
                
                
                
                    <li>17 庖丁解牛：kube-proxy</li>
                
                
                
                    <li><a href="http://yipsen.github.io/documents/columns/kubernetes/kubernetes%E4%BB%8E%E4%B8%8A%E6%89%8B%E5%88%B0%E5%AE%9E%E8%B7%B5/18-%E5%BA%96%E4%B8%81%E8%A7%A3%E7%89%9Bcontainer-runtime-docker/">18 庖丁解牛：Container Runtime （Docker）</a></li>
                
                
                
                    <li><a href="http://yipsen.github.io/documents/columns/kubernetes/kubernetes%E4%BB%8E%E4%B8%8A%E6%89%8B%E5%88%B0%E5%AE%9E%E8%B7%B5/19-troubleshoot/">19 Troubleshoot</a></li>
                
                
                
                    <li><a href="http://yipsen.github.io/documents/columns/kubernetes/kubernetes%E4%BB%8E%E4%B8%8A%E6%89%8B%E5%88%B0%E5%AE%9E%E8%B7%B5/20-%E6%89%A9%E5%B1%95%E5%A2%9E%E5%BC%BAdashboard/">20 扩展增强：Dashboard</a></li>
                
                
                
                    <li><a href="http://yipsen.github.io/documents/columns/kubernetes/kubernetes%E4%BB%8E%E4%B8%8A%E6%89%8B%E5%88%B0%E5%AE%9E%E8%B7%B5/21-%E6%89%A9%E5%B1%95%E5%A2%9E%E5%BC%BAcoredns/">21 扩展增强：CoreDNS</a></li>
                
                
                
                    <li><a href="http://yipsen.github.io/documents/columns/kubernetes/kubernetes%E4%BB%8E%E4%B8%8A%E6%89%8B%E5%88%B0%E5%AE%9E%E8%B7%B5/22-%E6%9C%8D%E5%8A%A1%E5%A2%9E%E5%BC%BAingress/">22 服务增强：Ingress</a></li>
                
                
                
                    <li><a href="http://yipsen.github.io/documents/columns/kubernetes/kubernetes%E4%BB%8E%E4%B8%8A%E6%89%8B%E5%88%B0%E5%AE%9E%E8%B7%B5/23-%E7%9B%91%E6%8E%A7%E5%AE%9E%E8%B7%B5%E5%AF%B9-k8s-%E9%9B%86%E7%BE%A4%E8%BF%9B%E8%A1%8C%E7%9B%91%E6%8E%A7/">23 监控实践：对 K8S 集群进行监控</a></li>
                
                
                
                    <li><a href="http://yipsen.github.io/documents/columns/kubernetes/kubernetes%E4%BB%8E%E4%B8%8A%E6%89%8B%E5%88%B0%E5%AE%9E%E8%B7%B5/24-%E6%80%BB%E7%BB%93/">24 总结</a></li>
                
                
            </ul>
        
    </div>
</aside>

<style>
    aside {
        position: fixed;
        left: 20px;
        background: #ffdee3;
        border-radius: 6px;
        padding: 20px;
        padding-top: 60px;
        box-shadow: 5px 10px #888;
        list-style-type: none;
        display: none;
    }

    .post-category-icon {
        position: absolute;
        width: 80px;
        height: 80px;
        top: -40px;
        left: 50%;
        margin-left: -40px;
        text-align: center;
        font-size: 36px;
        content: 'J';
        background: #ffdee3;
        border-radius: 100%;
        border: 10px solid #fff;
        box-shadow: 5px 10px #888;
    }
</style><article class="post-block">
        <h1 class="post-title">17 庖丁解牛：kube-proxy</h1>
        <div class="post-info">
            <div> 
                <span class="post-date">📅&nbsp;2021-12-22 01:47:43</span>🏷️&nbsp;<a class="post-tag" href="/tags/kubernetes/">kubernetes</a></div><div class="post-category">
                
                📚&nbsp;<a href="/categories/kubernetes%E4%BB%8E%E4%B8%8A%E6%89%8B%E5%88%B0%E5%AE%9E%E8%B7%B5/">Kubernetes从上手到实践</a>
                
            </div></div>
        <div class="post-content">
            <h2 id="整体概览">整体概览</h2>
<p>在第 3 节中，我们了解到 <code>kube-proxy</code> 的存在，而在第 7 中，我们学习到了如何将运行于 K8S 中的服务以 <code>Service</code> 的方式暴露出来，以供访问。</p>
<p>本节，我们来介绍下 <code>kube-proxy</code> 了解下它是如何支撑起这种类似服务发现和代理相关功能的。</p>
<h2 id="kube-proxy-是什么"><code>kube-proxy</code> 是什么</h2>
<p><code>kube-proxy</code> 是 K8S 运行于每个 <code>Node</code> 上的网络代理组件，提供了 TCP 和 UDP 的连接转发支持。</p>
<p>我们已经知道，当 <code>Pod</code> 在创建和销毁的过程中，IP 可能会发生变化，而这就容易造成对其有依赖的服务的异常，所以通常情况下，我们都会使用 <code>Service</code> 将后端 <code>Pod</code> 暴露出来，而 <code>Service</code> 则较为稳定。</p>
<p>还是以我们之前的 <a href="https://github.com/tao12345666333/saythx"><code>SayThx</code></a> 项目为例，但我们只部署其中没有任何依赖的后端资源 <code>Redis</code> 。</p>
<pre tabindex="0"><code>master $ git clone https://github.com/tao12345666333/saythx.git
Cloning into 'saythx'...
remote: Enumerating objects: 110, done.
remote: Counting objects: 100% (110/110), done.
remote: Compressing objects: 100% (82/82), done.
remote: Total 110 (delta 27), reused 102 (delta 20), pack-reused 0
Receiving objects: 100% (110/110), 119.42 KiB | 0 bytes/s, done.
Resolving deltas: 100% (27/27), done.
Checking connectivity... done.
master $ cd saythx/deploy
master $ ls
backend-deployment.yaml  frontend-deployment.yaml  namespace.yaml         redis-service.yaml
backend-service.yaml     frontend-service.yaml     redis-deployment.yaml  work-deployment.yaml
</code></pre><p>进入配置文件所在目录后，开始创建相关资源：</p>
<pre tabindex="0"><code>master $ kubectl apply -f namespace.yaml
namespace/work created
master $ kubectl apply -f redis-deployment.yaml
deployment.apps/saythx-redis created
master $ kubectl  apply -f redis-service.yaml
service/saythx-redis created
master $ kubectl -n work get all
NAME                               READY     STATUS    RESTARTS   AGE
pod/saythx-redis-8558c7d7d-wsn2w   1/1       Running   0          21s

NAME                   TYPE       CLUSTER-IP       EXTERNAL-IP   PORT(S)          AGE
service/saythx-redis   NodePort   10.103.193.175   &lt;none&gt;        6379:31269/TCP   6s

NAME                           DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/saythx-redis   1         1         1            1           21s

NAME                                     DESIRED   CURRENT   READY     AGE
replicaset.apps/saythx-redis-8558c7d7d   1         1         1         21s
</code></pre><p>可以看到 Redis 正在运行，并通过 <code>NodePort</code> 类型的 <code>Service</code> 暴露出来，我们访问来确认下。</p>
<pre tabindex="0"><code>master $ docker run --rm -it --network host redis:alpine redis-cli -p 31269
Unable to find image 'redis:alpine' locally
alpine: Pulling from library/redis
4fe2ade4980c: Already exists
fb758dc2e038: Pull complete
989f7b0c858b: Pull complete
8dd99d530347: Pull complete
7137334fa8f0: Pull complete
30610ca64487: Pull complete
Digest: sha256:8fd83c5986f444f1a5521e3eda7395f0f21ff16d33cc3b89d19ca7c58293c5dd
Status: Downloaded newer image for redis:alpine
127.0.0.1:31269&gt; set name kubernetes
OK
127.0.0.1:31269&gt; get name 
&quot;kubernetes&quot;
</code></pre><p>可以看到已经可以正常访问。接下来，我们来看下 <code>31269</code> 这个端口的状态。</p>
<pre tabindex="0"><code>master $ netstat  -ntlp |grep 31269
tcp6       0      0 :::31269                :::*                    LISTEN      2716/kube-proxy
</code></pre><p>可以看到该端口是由 <code>kube-proxy</code> 所占用的。</p>
<p>接下来，查看当前集群的 <code>Service</code> 和 <code>Endpoint</code></p>
<pre tabindex="0"><code>master $ kubectl -n work get svc
NAME           TYPE       CLUSTER-IP       EXTERNAL-IP   PORT(S)          AGE
saythx-redis   NodePort   10.103.193.175   &lt;none&gt;        6379:31269/TCP   10m
master $ kubectl -n work get endpoints
NAME           ENDPOINTS        AGE
saythx-redis   10.32.0.2:6379   10m
master $ kubectl -n work get pod -o wide
NAME                           READY     STATUS    RESTARTS   AGE       IP          NODE      NOMINATED NODE
saythx-redis-8558c7d7d-wsn2w   1/1       Running   0          12m       10.32.0.2   node01    &lt;none&gt;
</code></pre><p>可以很直观的看到 <code>Endpoint</code> 当中的便是 <code>Pod</code> 的 IP，现在我们将该服务进行扩容（实际情况下并不会这样处理）。</p>
<p>直接通过 <code>kubectl scale</code> 操作</p>
<pre tabindex="0"><code>master $ kubectl  -n work scale --replicas=2 deploy/saythx-redis
deployment.extensions/saythx-redis scaled
master $ kubectl  -n work get all
NAME                               READY     STATUS    RESTARTS   AGE
pod/saythx-redis-8558c7d7d-sslpj   1/1       Running   0          10s
pod/saythx-redis-8558c7d7d-wsn2w   1/1       Running   0          16m

NAME                   TYPE       CLUSTER-IP       EXTERNAL-IP   PORT(S)          AGE
service/saythx-redis   NodePort   10.103.193.175   &lt;none&gt;        6379:31269/TCP   16m

NAME                           DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/saythx-redis   2         2         2            2           16m
</code></pre><p>查看 <code>Endpoint</code> 信息：</p>
<pre tabindex="0"><code>master $ kubectl -n work get endpoints
NAME           ENDPOINTS                       AGE
saythx-redis   10.32.0.2:6379,10.32.0.3:6379   17m
</code></pre><p>可以看到 <code>Endpoint</code> 已经自动发生了变化，而这也意味着 <code>Service</code> 代理的后端节点将增加一个。</p>
<h2 id="kube-proxy-如何工作"><code>kube-proxy</code> 如何工作</h2>
<p><code>kube-proxy</code> 在 Linux 系统上当前支持三种模式，可通过 <code>--proxy-mode</code> 配置：</p>
<ul>
<li><code>userspace</code>：这是很早期的一种方案，但效率上显著不足，不推荐使用。</li>
<li><code>iptables</code>：当前的默认模式。比 <code>userspace</code> 要快，但问题是会给机器上产生很多 <code>iptables</code> 规则。</li>
<li><code>ipvs</code>：为了解决 <code>iptables</code> 的性能问题而引入，采用增量的方式进行更新。</li>
</ul>
<p>下面我们以 <code>iptables</code> 的模式稍作介绍。</p>
<pre tabindex="0"><code>master $ iptables -t nat -L 
Chain PREROUTING (policy ACCEPT)
target     prot opt source               destination
KUBE-SERVICES  all  --  anywhere             anywhere             /* kubernetes service portals */
DOCKER     all  --  anywhere             anywhere             ADDRTYPE match dst-type LOCAL

Chain INPUT (policy ACCEPT)
target     prot opt source               destination

Chain OUTPUT (policy ACCEPT)
target     prot opt source               destination
KUBE-SERVICES  all  --  anywhere             anywhere             /* kubernetes service portals */
DOCKER     all  --  anywhere            !127.0.0.0/8          ADDRTYPE match dst-type LOCAL

Chain POSTROUTING (policy ACCEPT)
target     prot opt source               destination
KUBE-POSTROUTING  all  --  anywhere             anywhere             /* kubernetes postrouting rules */
MASQUERADE  all  --  172.18.0.0/24        anywhere

Chain DOCKER (2 references)
target     prot opt source               destination
RETURN     all  --  anywhere             anywhere

Chain KUBE-MARK-DROP (0 references)
target     prot opt source               destination
MARK       all  --  anywhere             anywhere             MARK or 0x8000

Chain KUBE-MARK-MASQ (7 references)
target     prot opt source               destination
MARK       all  --  anywhere             anywhere             MARK or 0x4000

Chain KUBE-NODEPORTS (1 references)
target     prot opt source               destination
KUBE-MARK-MASQ  tcp  --  anywhere             anywhere             /* work/saythx-redis: */ tcp dpt:31269
KUBE-SVC-SMQNAAUIAENDDGYQ  tcp  --  anywhere             anywhere             /* work/saythx-redis: */ tcp dpt:31269

Chain KUBE-POSTROUTING (1 references)
target     prot opt source               destination
MASQUERADE  all  --  anywhere             anywhere             /* kubernetes service traffic requiring SNAT */ mark match 0x4000/0x4000

Chain KUBE-SEP-2LZPYBS4HUAJKDFL (1 references)
target     prot opt source               destination
KUBE-MARK-MASQ  all  --  10.32.0.2            anywhere             /* kube-system/kube-dns:dns-tcp */
DNAT       tcp  --  anywhere             anywhere             /* kube-system/kube-dns:dns-tcp */ tcp to:10.32.0.2:53

Chain KUBE-SEP-3E4LNQKKWZF7G6SH (1 references)
target     prot opt source               destination
KUBE-MARK-MASQ  all  --  10.32.0.1            anywhere             /* kube-system/kube-dns:dns-tcp */
DNAT       tcp  --  anywhere             anywhere             /* kube-system/kube-dns:dns-tcp */ tcp to:10.32.0.1:53

Chain KUBE-SEP-3IDG7DUGN3QC2UZF (1 references)
target     prot opt source               destination
KUBE-MARK-MASQ  all  --  172.17.0.120         anywhere             /* default/kubernetes:https */
DNAT       tcp  --  anywhere             anywhere             /* default/kubernetes:https */ tcp to:172.17.0.120:6443

Chain KUBE-SEP-JZWS2VPNIEMNMNB2 (1 references)
target     prot opt source               destination
KUBE-MARK-MASQ  all  --  10.32.0.2            anywhere             /* kube-system/kube-dns:dns */
DNAT       udp  --  anywhere             anywhere             /* kube-system/kube-dns:dns */ udp to:10.32.0.2:53

Chain KUBE-SEP-OEY6JJQSBCQPRKHS (1 references)
target     prot opt source               destination
KUBE-MARK-MASQ  all  --  10.32.0.1            anywhere             /* kube-system/kube-dns:dns */
DNAT       udp  --  anywhere             anywhere             /* kube-system/kube-dns:dns */ udp to:10.32.0.1:53

Chain KUBE-SEP-QX7VDAS5KDY6V3EV (1 references)
target     prot opt source               destination
KUBE-MARK-MASQ  all  --  10.32.0.2            anywhere             /* work/saythx-redis: */
DNAT       tcp  --  anywhere             anywhere             /* work/saythx-redis: */ tcp to:10.32.0.2:6379

Chain KUBE-SERVICES (2 references)
target     prot opt source               destination
KUBE-SVC-SMQNAAUIAENDDGYQ  tcp  --  anywhere             10.103.193.175       /* work/saythx-redis: cluster IP */ tcp dpt:6379
KUBE-NODEPORTS  all  --  anywhere             anywhere             /* kubernetes service nodeports; NOTE: this must be the last rule in this chain */ ADDRTYPE match dst-type LOCAL

Chain KUBE-SVC-ERIFXISQEP7F7OF4 (1 references)
target     prot opt source               destination
KUBE-SEP-3E4LNQKKWZF7G6SH  all  --  anywhere             anywhere             /* kube-system/kube-dns:dns-tcp */ statistic mode random probability 0.50000000000
KUBE-SEP-2LZPYBS4HUAJKDFL  all  --  anywhere             anywhere             /* kube-system/kube-dns:dns-tcp */

Chain KUBE-SVC-SMQNAAUIAENDDGYQ (2 references)
target     prot opt source               destination
KUBE-SEP-QX7VDAS5KDY6V3EV  all  --  anywhere             anywhere             /* work/saythx-redis: */
</code></pre><p>以上输出已经尽量删掉了无关的内容。</p>
<p>当开始访问的时候先要经过 <code>PREROUTING</code> 链，转到 <code>KUBE-SERVICES</code> 链，当查询到匹配的规则之后，请求将转向 <code>KUBE-SVC-SMQNAAUIAENDDGYQ</code> 链，进而到达 <code>KUBE-SEP-QX7VDAS5KDY6V3EV</code> 对应于我们的 <code>Pod</code>。(注：为了简洁，上述 iptables 规则是部署一个 <code>Pod</code> 时的场景)</p>
<p>当搞懂了这些之后，如果你想了解这些 <code>iptables</code> 规则实际又是如何创建和维护的，那可以参考下 <code>proxier</code> 的具体实现，这里不再展开。</p>
<h2 id="总结">总结</h2>
<p>本节中我们介绍了 <code>kube-proxy</code> 的主要功能和基本流程，了解到了它对于服务注册发现和代理访问等起到了很大的作用。而它在 Linux 下的代理模式也有 <code>userspace</code>，<code>iptables</code> 和 <code>ipvs</code> 等。</p>
<p>默认情况下我们使用 <code>iptables</code> 的代理模式，当创建新的 <code>Service</code> ，或者 <code>Pod</code> 进行变化时，<code>kube-proxy</code> 便会去维护 <code>iptables</code> 规则，以确保请求可以正确的到达后端服务。</p>
<p>当然，本节中并没有提到 <code>kube-proxy</code> 的 <code>session affinity</code> 相关的特性，如有需要可进行下尝试。</p>
<p>下节，我们将介绍实际运行着容器的 <code>Docker</code>，大致了解下在 K8S 中它所起的作用，及他们之间的交互方式。</p>

        </div>
    </article>
</div>

    </main>
    <footer>
        <div id="pagination">
            
<div class="paginator">
    <div class="prev">
        
        <a href="/documents/columns/kubernetes/kubernetes%E4%BB%8E%E4%B8%8A%E6%89%8B%E5%88%B0%E5%AE%9E%E8%B7%B5/16-%E5%BA%96%E4%B8%81%E8%A7%A3%E7%89%9Bkubelet/"><span>16 庖丁解牛：kubelet</span></a>
    </div>
    <div class="next">
        
        <a href="/documents/columns/kubernetes/kubernetes%E4%BB%8E%E4%B8%8A%E6%89%8B%E5%88%B0%E5%AE%9E%E8%B7%B5/18-%E5%BA%96%E4%B8%81%E8%A7%A3%E7%89%9Bcontainer-runtime-docker/"><span>18 庖丁解牛：Container Runtime （Docker）</span></a>
    </div>
</div>

        </div>
        <div class="toggler" onclick="toggleLight(this);">💡</div>
        <div id="copyright" style="display: none;">
    
    
    <p>&copy; 27270 <a href="/"></a>, powered by Hugo and Qiao</p>
</div>
    </footer>
    
<script>
    function toggleLight() {
        document.getElementById('page').classList.toggle('night');
    }
</script>
</body>

</html>