<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="utf-8">
<meta name="X-UA-Compatible" , content="IE=edge,chrome=1">
<meta name="viewport" content="width=device-width,initial-scale=1.0">

<link rel="icon" type="image/png" href="/%20/favicon.png">

<title>23 K8s 集群中存储对象灾备的落地实践 | Yipsen Ye</title>
<meta name="description" content="谈到存储对象的灾备，我们可以想象成当你启动了挂载卷的 Pod 的时候，突然集群机器宕机的场景，我们应该如何应对存储对象的容错能力呢？应用的高可用固然最好，但是灾备方案一直都是最后一道门槛，在很多极限情况下，容错的备份是你安心提供服务的保障。
在虚拟机时代，我们通过控制应用平均分配到各个虚拟机中和定期计划执行的数据备份，让业务可靠性不断地提高。现在升级到 Kubernetes 时代，所有业务都被 Kubernetes 托管，集群可以迅速调度并自维护应用的容器状态，随时可以扩缩资源来应对突发情况。
听笔者这么说，感觉好像并不需要对存储有多大的担心，只要挂载的是网络存储，即使应用集群坏了，数据还在么，好像也没有多大的事情，那么学这个存储对象的灾备又有什么意义呢？
笔者想说事情远没有想象中那么简单，我们需要带入接近业务的场景中，再来通过破坏集群状态，看看读存储对象是否有破坏性。
因为我们从虚拟机时代升级到 Kubernetes 时代，我们的目的是利用动态扩缩的资源来减少业务中断的时间，让应用可以随需扩缩，随需自愈。所以在 Kubernetes 时代，我们要的并不是数据丢不丢的问题，而是能不能有快速保障让业务恢复时间越来越短，甚至让用户没有感知。这个可能实现吗？
笔者认为 Kubernetes 通过不断丰富的资源对象已经快接近实现这个目标了。所以笔者这里带着大家一起梳理一遍各种存储对象的灾备在 Kubernetes 落地的实践经验，以备不时之需。
NFS 存储对象的灾备落地经验 首先我们应该理解 PV/PVC 创建 NFS 网络卷的配置方法，注意 mountOptions 参数的使用姿势。如下例子参考：
### nfs-pv.yamlapiVersion: v1kind: PersistentVolumemetadata:name: nfs-pvspec:capacity:storage: 10GivolumeMode: FilesystemaccessModes:- ReadWriteManypersistentVolumeReclaimPolicy: RecyclestorageClassName: nfsmountOptions:- hard- nfsvers=4.1nfs:path: /opt/k8s-pods/data # 指定 nfs 的挂载点server: 192.168.1.40 # 指定 nfs 服务地址---### nfs-pvc.">
<meta name="author" content="">

<link rel="stylesheet" type="text/css" href="/styles/main.css">

</head>

<body id="page">
    <header>
        <div class="logo-link" style="width: 60px; color: #CA6924;">
    <a href="http://yipsen.github.io/">
        <img style="margin-bottom: -1rem;" src="/images/deer.svg" alt="logo">
        <span style="font-size: 0.5rem; color: #CA6924;">Yipsen Ye</span>
    </a>
</div>
<nav class="navbar justify-content-end">
    <ul class="nav-list">
        
        
        <li class="nav-item">
            <a class="nav-link"
                href="http://yipsen.github.io/posts/">札记</a>
        </li>
        
        <li class="nav-item">
            <a class="nav-link"
                href="http://yipsen.github.io/collections/">附表</a>
        </li>
        
        <li class="nav-item">
            <a class="nav-link"
                href="http://yipsen.github.io/about/">关于</a>
        </li>
        
    </ul>
</nav>
    </header>
    <main id="content">
        
<div class="container"><aside>
    <div>
        
        
            
            
            <div class="post-category-icon"></div>
            <a href="/categories/kubernetes%e5%ae%9e%e8%b7%b5%e5%85%a5%e9%97%a8%e6%8c%87%e5%8d%97/">Kubernetes实践入门指南</a>
            <ul>
                
                
                    <li><a href="http://yipsen.github.io/documents/columns/kubernetes/kubernetes%E5%AE%9E%E8%B7%B5%E5%85%A5%E9%97%A8%E6%8C%87%E5%8D%97/00-%E4%B8%BA%E4%BB%80%E4%B9%88%E6%88%91%E4%BB%AC%E8%A6%81%E5%AD%A6%E4%B9%A0-kubernetes-%E6%8A%80%E6%9C%AF/">00 为什么我们要学习 Kubernetes 技术</a></li>
                
                
                
                    <li><a href="http://yipsen.github.io/documents/columns/kubernetes/kubernetes%E5%AE%9E%E8%B7%B5%E5%85%A5%E9%97%A8%E6%8C%87%E5%8D%97/01-%E9%87%8D%E6%96%B0%E8%AE%A4%E8%AF%86-kubernetes-%E7%9A%84%E6%A0%B8%E5%BF%83%E7%BB%84%E4%BB%B6/">01 重新认识 Kubernetes 的核心组件</a></li>
                
                
                
                    <li><a href="http://yipsen.github.io/documents/columns/kubernetes/kubernetes%E5%AE%9E%E8%B7%B5%E5%85%A5%E9%97%A8%E6%8C%87%E5%8D%97/02-%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3-kubernets-%E7%9A%84%E7%BC%96%E6%8E%92%E5%AF%B9%E8%B1%A1/">02 深入理解 Kubernets 的编排对象</a></li>
                
                
                
                    <li><a href="http://yipsen.github.io/documents/columns/kubernetes/kubernetes%E5%AE%9E%E8%B7%B5%E5%85%A5%E9%97%A8%E6%8C%87%E5%8D%97/03-devops-%E5%9C%BA%E6%99%AF%E4%B8%8B%E8%90%BD%E5%9C%B0-k8s-%E7%9A%84%E5%9B%B0%E9%9A%BE%E5%88%86%E6%9E%90/">03 DevOps 场景下落地 K8s 的困难分析</a></li>
                
                
                
                    <li><a href="http://yipsen.github.io/documents/columns/kubernetes/kubernetes%E5%AE%9E%E8%B7%B5%E5%85%A5%E9%97%A8%E6%8C%87%E5%8D%97/04-%E5%BE%AE%E6%9C%8D%E5%8A%A1%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF%E4%B8%8B%E8%90%BD%E5%9C%B0-k8s-%E7%9A%84%E5%9B%B0%E9%9A%BE%E5%88%86%E6%9E%90/">04 微服务应用场景下落地 K8s 的困难分析</a></li>
                
                
                
                    <li><a href="http://yipsen.github.io/documents/columns/kubernetes/kubernetes%E5%AE%9E%E8%B7%B5%E5%85%A5%E9%97%A8%E6%8C%87%E5%8D%97/05-%E8%A7%A3%E5%86%B3-k8s-%E8%90%BD%E5%9C%B0%E9%9A%BE%E9%A2%98%E7%9A%84%E6%96%B9%E6%B3%95%E8%AE%BA%E6%8F%90%E7%82%BC/">05 解决 K8s 落地难题的方法论提炼</a></li>
                
                
                
                    <li><a href="http://yipsen.github.io/documents/columns/kubernetes/kubernetes%E5%AE%9E%E8%B7%B5%E5%85%A5%E9%97%A8%E6%8C%87%E5%8D%97/06-%E7%BB%83%E4%B9%A0%E7%AF%87k8s-%E6%A0%B8%E5%BF%83%E5%AE%9E%E8%B7%B5%E7%9F%A5%E8%AF%86%E6%8E%8C%E6%8F%A1/">06 练习篇：K8s 核心实践知识掌握</a></li>
                
                
                
                    <li><a href="http://yipsen.github.io/documents/columns/kubernetes/kubernetes%E5%AE%9E%E8%B7%B5%E5%85%A5%E9%97%A8%E6%8C%87%E5%8D%97/07-%E5%AE%B9%E5%99%A8%E5%BC%95%E6%93%8E-containerd-%E8%90%BD%E5%9C%B0%E5%AE%9E%E8%B7%B5/">07 容器引擎 containerd 落地实践</a></li>
                
                
                
                    <li><a href="http://yipsen.github.io/documents/columns/kubernetes/kubernetes%E5%AE%9E%E8%B7%B5%E5%85%A5%E9%97%A8%E6%8C%87%E5%8D%97/08-k8s-%E9%9B%86%E7%BE%A4%E5%AE%89%E8%A3%85%E5%B7%A5%E5%85%B7-kubeadm-%E7%9A%84%E8%90%BD%E5%9C%B0%E5%AE%9E%E8%B7%B5/">08 K8s 集群安装工具 kubeadm 的落地实践</a></li>
                
                
                
                    <li><a href="http://yipsen.github.io/documents/columns/kubernetes/kubernetes%E5%AE%9E%E8%B7%B5%E5%85%A5%E9%97%A8%E6%8C%87%E5%8D%97/09-%E5%8D%97%E5%8C%97%E5%90%91%E6%B5%81%E9%87%8F%E7%BB%84%E4%BB%B6-ipvs-%E7%9A%84%E8%90%BD%E5%9C%B0%E5%AE%9E%E8%B7%B5/">09 南北向流量组件 IPVS 的落地实践</a></li>
                
                
                
                    <li><a href="http://yipsen.github.io/documents/columns/kubernetes/kubernetes%E5%AE%9E%E8%B7%B5%E5%85%A5%E9%97%A8%E6%8C%87%E5%8D%97/10-%E4%B8%9C%E8%A5%BF%E5%90%91%E6%B5%81%E9%87%8F%E7%BB%84%E4%BB%B6-calico-%E7%9A%84%E8%90%BD%E5%9C%B0%E5%AE%9E%E8%B7%B5/">10 东西向流量组件 Calico 的落地实践</a></li>
                
                
                
                    <li><a href="http://yipsen.github.io/documents/columns/kubernetes/kubernetes%E5%AE%9E%E8%B7%B5%E5%85%A5%E9%97%A8%E6%8C%87%E5%8D%97/11-%E6%9C%8D%E5%8A%A1%E5%8F%91%E7%8E%B0-dns-%E7%9A%84%E8%90%BD%E5%9C%B0%E5%AE%9E%E8%B7%B5/">11 服务发现 DNS 的落地实践</a></li>
                
                
                
                    <li><a href="http://yipsen.github.io/documents/columns/kubernetes/kubernetes%E5%AE%9E%E8%B7%B5%E5%85%A5%E9%97%A8%E6%8C%87%E5%8D%97/12-%E7%BB%83%E4%B9%A0%E7%AF%87k8s-%E9%9B%86%E7%BE%A4%E9%85%8D%E7%BD%AE%E6%B5%8B%E9%AA%8C/">12 练习篇：K8s 集群配置测验</a></li>
                
                
                
                    <li><a href="http://yipsen.github.io/documents/columns/kubernetes/kubernetes%E5%AE%9E%E8%B7%B5%E5%85%A5%E9%97%A8%E6%8C%87%E5%8D%97/13-%E7%90%86%E8%A7%A3%E5%AF%B9%E6%96%B9%E6%9A%B4%E9%9C%B2%E6%9C%8D%E5%8A%A1%E7%9A%84%E5%AF%B9%E8%B1%A1-ingress-%E5%92%8C-service/">13 理解对方暴露服务的对象 Ingress 和 Service</a></li>
                
                
                
                    <li><a href="http://yipsen.github.io/documents/columns/kubernetes/kubernetes%E5%AE%9E%E8%B7%B5%E5%85%A5%E9%97%A8%E6%8C%87%E5%8D%97/14-%E5%BA%94%E7%94%A8%E7%BD%91%E5%85%B3-openresty-%E5%AF%B9%E6%8E%A5-k8s-%E5%AE%9E%E8%B7%B5/">14 应用网关 OpenResty 对接 K8s 实践</a></li>
                
                
                
                    <li><a href="http://yipsen.github.io/documents/columns/kubernetes/kubernetes%E5%AE%9E%E8%B7%B5%E5%85%A5%E9%97%A8%E6%8C%87%E5%8D%97/15-service-%E5%B1%82%E5%BC%95%E6%B5%81%E6%8A%80%E6%9C%AF%E5%AE%9E%E8%B7%B5/">15 Service 层引流技术实践</a></li>
                
                
                
                    <li><a href="http://yipsen.github.io/documents/columns/kubernetes/kubernetes%E5%AE%9E%E8%B7%B5%E5%85%A5%E9%97%A8%E6%8C%87%E5%8D%97/16-cilium-%E5%AE%B9%E5%99%A8%E7%BD%91%E7%BB%9C%E7%9A%84%E8%90%BD%E5%9C%B0%E5%AE%9E%E8%B7%B5/">16 Cilium 容器网络的落地实践</a></li>
                
                
                
                    <li><a href="http://yipsen.github.io/documents/columns/kubernetes/kubernetes%E5%AE%9E%E8%B7%B5%E5%85%A5%E9%97%A8%E6%8C%87%E5%8D%97/17-%E5%BA%94%E7%94%A8%E6%B5%81%E9%87%8F%E7%9A%84%E4%BC%98%E9%9B%85%E6%97%A0%E6%8D%9F%E5%88%87%E6%8D%A2%E5%AE%9E%E8%B7%B5/">17 应用流量的优雅无损切换实践</a></li>
                
                
                
                    <li><a href="http://yipsen.github.io/documents/columns/kubernetes/kubernetes%E5%AE%9E%E8%B7%B5%E5%85%A5%E9%97%A8%E6%8C%87%E5%8D%97/18-%E7%BB%83%E4%B9%A0%E7%AF%87%E5%BA%94%E7%94%A8%E6%B5%81%E9%87%8F%E6%97%A0%E6%8D%9F%E5%88%87%E6%8D%A2%E6%8A%80%E6%9C%AF%E6%B5%8B%E9%AA%8C/">18 练习篇：应用流量无损切换技术测验</a></li>
                
                
                
                    <li><a href="http://yipsen.github.io/documents/columns/kubernetes/kubernetes%E5%AE%9E%E8%B7%B5%E5%85%A5%E9%97%A8%E6%8C%87%E5%8D%97/19-%E4%BD%BF%E7%94%A8-rook-%E6%9E%84%E5%BB%BA%E7%94%9F%E4%BA%A7%E5%8F%AF%E7%94%A8%E5%AD%98%E5%82%A8%E7%8E%AF%E5%A2%83%E5%AE%9E%E8%B7%B5/">19 使用 Rook 构建生产可用存储环境实践</a></li>
                
                
                
                    <li><a href="http://yipsen.github.io/documents/columns/kubernetes/kubernetes%E5%AE%9E%E8%B7%B5%E5%85%A5%E9%97%A8%E6%8C%87%E5%8D%97/20-%E6%9C%89%E7%8A%B6%E6%80%81%E5%BA%94%E7%94%A8%E7%9A%84%E9%BB%98%E8%AE%A4%E7%89%B9%E6%80%A7%E8%90%BD%E5%9C%B0%E5%88%86%E6%9E%90/">20 有状态应用的默认特性落地分析</a></li>
                
                
                
                    <li><a href="http://yipsen.github.io/documents/columns/kubernetes/kubernetes%E5%AE%9E%E8%B7%B5%E5%85%A5%E9%97%A8%E6%8C%87%E5%8D%97/21-%E6%A1%88%E4%BE%8B%E5%88%86%E5%B8%83%E5%BC%8F-mysql-%E9%9B%86%E7%BE%A4%E5%B7%A5%E5%85%B7-vitess-%E5%AE%9E%E8%B7%B5%E5%88%86%E6%9E%90/">21 案例：分布式 MySQL 集群工具 Vitess 实践分析</a></li>
                
                
                
                    <li><a href="http://yipsen.github.io/documents/columns/kubernetes/kubernetes%E5%AE%9E%E8%B7%B5%E5%85%A5%E9%97%A8%E6%8C%87%E5%8D%97/22-%E5%AD%98%E5%82%A8%E5%AF%B9%E8%B1%A1-pvpvcstorage-classes-%E7%9A%84%E7%AE%A1%E7%90%86%E8%90%BD%E5%9C%B0%E5%AE%9E%E8%B7%B5/">22 存储对象 PV、PVC、Storage Classes 的管理落地实践</a></li>
                
                
                
                    <li>23 K8s 集群中存储对象灾备的落地实践</li>
                
                
                
                    <li><a href="http://yipsen.github.io/documents/columns/kubernetes/kubernetes%E5%AE%9E%E8%B7%B5%E5%85%A5%E9%97%A8%E6%8C%87%E5%8D%97/24-%E7%BB%83%E4%B9%A0%E7%AF%87k8s-%E9%9B%86%E7%BE%A4%E9%85%8D%E7%BD%AE%E6%B5%8B%E9%AA%8C/">24 练习篇：K8s 集群配置测验</a></li>
                
                
            </ul>
        
    </div>
</aside>

<style>
    aside {
        position: fixed;
        left: 20px;
        background: #ffdee3;
        border-radius: 6px;
        padding: 20px;
        padding-top: 60px;
        box-shadow: 5px 10px #888;
        list-style-type: none;
        display: none;
    }

    .post-category-icon {
        position: absolute;
        width: 80px;
        height: 80px;
        top: -40px;
        left: 50%;
        margin-left: -40px;
        text-align: center;
        font-size: 36px;
        content: 'J';
        background: #ffdee3;
        border-radius: 100%;
        border: 10px solid #fff;
        box-shadow: 5px 10px #888;
    }
</style><article class="post-block">
        <h1 class="post-title">23 K8s 集群中存储对象灾备的落地实践</h1>
        <div class="post-info">
            <div> 
                <span class="post-date">📅&nbsp;2021-12-22 01:48:29</span>🏷️&nbsp;<a class="post-tag" href="/tags/kubernetes/">kubernetes</a></div><div class="post-category">
                
                📚&nbsp;<a href="/categories/kubernetes%E5%AE%9E%E8%B7%B5%E5%85%A5%E9%97%A8%E6%8C%87%E5%8D%97/">Kubernetes实践入门指南</a>
                
            </div></div>
        <div class="post-content">
            <p>谈到存储对象的灾备，我们可以想象成当你启动了挂载卷的 Pod 的时候，突然集群机器宕机的场景，我们应该如何应对存储对象的容错能力呢？应用的高可用固然最好，但是灾备方案一直都是最后一道门槛，在很多极限情况下，容错的备份是你安心提供服务的保障。</p>
<p>在虚拟机时代，我们通过控制应用平均分配到各个虚拟机中和定期计划执行的数据备份，让业务可靠性不断地提高。现在升级到 Kubernetes 时代，所有业务都被 Kubernetes 托管，集群可以迅速调度并自维护应用的容器状态，随时可以扩缩资源来应对突发情况。</p>
<p>听笔者这么说，感觉好像并不需要对存储有多大的担心，只要挂载的是网络存储，即使应用集群坏了，数据还在么，好像也没有多大的事情，那么学这个存储对象的灾备又有什么意义呢？</p>
<p>笔者想说事情远没有想象中那么简单，我们需要带入接近业务的场景中，再来通过破坏集群状态，看看读存储对象是否有破坏性。</p>
<p>因为我们从虚拟机时代升级到 Kubernetes 时代，我们的目的是利用动态扩缩的资源来减少业务中断的时间，让应用可以随需扩缩，随需自愈。所以在 Kubernetes 时代，我们要的并不是数据丢不丢的问题，而是能不能有快速保障让业务恢复时间越来越短，甚至让用户没有感知。这个可能实现吗？</p>
<p>笔者认为 Kubernetes 通过不断丰富的资源对象已经快接近实现这个目标了。所以笔者这里带着大家一起梳理一遍各种存储对象的灾备在 Kubernetes 落地的实践经验，以备不时之需。</p>
<h3 id="nfs-存储对象的灾备落地经验">NFS 存储对象的灾备落地经验</h3>
<p>首先我们应该理解 PV/PVC 创建 NFS 网络卷的配置方法，注意 mountOptions 参数的使用姿势。如下例子参考：</p>
<pre tabindex="0"><code>### nfs-pv.yaml
apiVersion: v1
kind: PersistentVolume
metadata:
  name: nfs-pv
spec:
  capacity:
    storage: 10Gi
  volumeMode: Filesystem
  accessModes:
    - ReadWriteMany
  persistentVolumeReclaimPolicy: Recycle
  storageClassName: nfs
  mountOptions:
    - hard
    - nfsvers=4.1
  nfs:
    path: /opt/k8s-pods/data   # 指定 nfs 的挂载点
    server: 192.168.1.40  # 指定 nfs 服务地址
---
### nfs-pvc.yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: nfs-pvc
spec:
  storageClassName: nfs
  accessModes:
    - ReadWriteMany
  resources:
    requests:
      storage: 10Gi

</code></pre><p>在这个例子中，PersistentVolume 是 NFS 类型的，因此需要辅助程序 /sbin/mount.nfs 来支持挂载 NFS 文件系统。</p>
<pre tabindex="0"><code>[kadmin@k8s-master ~]$ kubectl get pvc nfs-pvc
NAME      STATUS   VOLUME   CAPACITY   ACCESS MODES   STORAGECLASS   AGE
nfs-pvc   Bound    nfs-pv   10Gi       RWX            nfs            3m54s
[kadmin@k8s-master ~]$
[kadmin@k8s-master ~]$ kubectl get pv nfs-pv
NAME     CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS   CLAIM             STORAGECLASS   REASON   AGE
nfs-pv   10Gi       RWX            Recycle          Bound    default/nfs-pvc   nfs                     18m

</code></pre><p>执行一个 Pod 挂载 NFS 卷：</p>
<pre tabindex="0"><code>### nfs-pv-pod.yaml
apiVersion: v1
kind: Pod
metadata:
  name: nginx-pv-pod
spec:
  volumes:
    - name: nginx-pv-storage
      persistentVolumeClaim:
        claimName: nfs-pvc
  containers:
    - name: nginx
      image: nginx
      ports:
        - containerPort: 80
          name: &quot;nginx-server&quot;
      volumeMounts:
        - mountPath: &quot;/usr/share/nginx/html&quot;
          name: nginx-pv-storage

复制
[kadmin@k8s-master ~]$ kubectl create -f nfs-pv-pod.yaml
pod/nginx-pv-pod created
[kadmin@k8s-master ~]$
[kadmin@k8s-master ~]$ kubectl get pod nginx-pv-pod -o wide
NAME           READY   STATUS    RESTARTS   AGE   IP              NODE           NOMINATED NODE   READINESS GATES
nginx-pv-pod   1/1     Running   0          66s   172.16.140.28   k8s-worker-2   &lt;none&gt;           &lt;none&gt;

[kadmin@k8s-master ~]$ curl http://172.16.140.28
Hello, NFS Storage NGINX

</code></pre><p>当你在一个 Pod 里面挂载了 NFS 卷之后，就需要考虑如何把数据备份出来。<a href="https://github.com/vmware-tanzu/velero">velero</a> 作为云原生的备份恢复工具出现了，它可以帮助我们备份持久化数据对象。velero 案例如下：</p>
<pre tabindex="0"><code>velero backup create backupName --include-cluster-resources=true --ordered-resources 'pods=ns1/pod1,ns1/pod2;persistentvolumes=pv4,pv8' --include-namespaces=ns1

</code></pre><p>注意 velero 默认没法备份卷，所以它集成了开源组件 <a href="https://github.com/restic/restic">restic</a> 支持了存储卷的支持。因为目前还处于试验阶段，注意请不要在生产环境中使用。</p>
<h3 id="ceph-数据备份及恢复">Ceph 数据备份及恢复</h3>
<p>Rook 是管理 Ceph 集群的云原生管理系统，在早前的课程中我已经和大家实践过使用 Rook 创建 Ceph 集群的方法。现在假设 Ceph 集群瘫痪了应该如何修复它。是的，我们需要手工修复它。步骤如下：</p>
<p>第一步，停止 Ceph operator 把 Ceph 集群的控制器关掉，不让它能自动负载自己的程序。</p>
<pre tabindex="0"><code>kubectl -n rook-ceph scale deployment rook-ceph-operator --replicas=0

</code></pre><p>第二步，这个 Ceph 的 monmap 保持跟踪 Ceph 节点的容错数量。我们先通过更新保持健康监控节点的实例正常运行。此处为 rook-ceph-mon-b，不健康的实例为 rook-ceph-mon-a 和 rook-ceph-mon-c。备份 rook-ceph-mon-b 的 Deployment 对象：</p>
<pre tabindex="0"><code>kubectl -n rook-ceph get deployment rook-ceph-mon-b -o yaml &gt; rook-ceph-mon-b-deployment.yaml

</code></pre><p>修改监控实例的命令：</p>
<pre tabindex="0"><code>kubectl -n rook-ceph patch deployment rook-ceph-mon-b -p '{&quot;spec&quot;: {&quot;template&quot;: {&quot;spec&quot;: {&quot;containers&quot;: [{&quot;name&quot;: &quot;mon&quot;, &quot;command&quot;: [&quot;sleep&quot;, &quot;infinity&quot;], &quot;args&quot;: []}]}}}}'

</code></pre><p>进入健康的监控实例中：</p>
<pre tabindex="0"><code>kubectl -n rook-ceph exec -it &lt;mon-pod&gt; bash

# set a few simple variables
cluster_namespace=rook-ceph
good_mon_id=b
monmap_path=/tmp/monmap

# extract the monmap to a file, by pasting the ceph mon command
# from the good mon deployment and adding the
# `--extract-monmap=${monmap_path}` flag
ceph-mon \
    --fsid=41a537f2-f282-428e-989f-a9e07be32e47 \
    --keyring=/etc/ceph/keyring-store/keyring \
    --log-to-stderr=true \
    --err-to-stderr=true \
    --mon-cluster-log-to-stderr=true \
    --log-stderr-prefix=debug \
    --default-log-to-file=false \
    --default-mon-cluster-log-to-file=false \
    --mon-host=$ROOK_CEPH_MON_HOST \
    --mon-initial-members=$ROOK_CEPH_MON_INITIAL_MEMBERS \
    --id=b \
    --setuser=ceph \
    --setgroup=ceph \
    --foreground \
    --public-addr=10.100.13.242 \
    --setuser-match-path=/var/lib/ceph/mon/ceph-b/store.db \
    --public-bind-addr=$ROOK_POD_IP \
    --extract-monmap=${monmap_path}

# review the contents of the monmap
monmaptool --print /tmp/monmap

# remove the bad mon(s) from the monmap
monmaptool ${monmap_path} --rm &lt;bad_mon&gt;

# in this example we remove mon0 and mon2:
monmaptool ${monmap_path} --rm a
monmaptool ${monmap_path} --rm c

# inject the modified monmap into the good mon, by pasting
# the ceph mon command and adding the
# `--inject-monmap=${monmap_path}` flag, like this
ceph-mon \
    --fsid=41a537f2-f282-428e-989f-a9e07be32e47 \
    --keyring=/etc/ceph/keyring-store/keyring \
    --log-to-stderr=true \
    --err-to-stderr=true \
    --mon-cluster-log-to-stderr=true \
    --log-stderr-prefix=debug \
    --default-log-to-file=false \
    --default-mon-cluster-log-to-file=false \
    --mon-host=$ROOK_CEPH_MON_HOST \
    --mon-initial-members=$ROOK_CEPH_MON_INITIAL_MEMBERS \
    --id=b \
    --setuser=ceph \
    --setgroup=ceph \
    --foreground \
    --public-addr=10.100.13.242 \
    --setuser-match-path=/var/lib/ceph/mon/ceph-b/store.db \
    --public-bind-addr=$ROOK_POD_IP \
    --inject-monmap=${monmap_path}

</code></pre><p>编辑 rook configmap 文件：</p>
<pre tabindex="0"><code>kubectl -n rook-ceph edit configmap rook-ceph-mon-endpoints

</code></pre><p>在 data 字段那里去掉过期的 a 和 b：</p>
<pre tabindex="0"><code>data: a=10.100.35.200:6789;b=10.100.13.242:6789;c=10.100.35.12:6789

</code></pre><p>变成：</p>
<pre tabindex="0"><code>data: b=10.100.13.242:6789

</code></pre><p>更新 secret 配置：</p>
<pre tabindex="0"><code>mon_host=$(kubectl -n rook-ceph get svc rook-ceph-mon-b -o jsonpath='{.spec.clusterIP}')
kubectl -n rook-ceph patch secret rook-ceph-config -p '{&quot;stringData&quot;: {&quot;mon_host&quot;: &quot;[v2:'&quot;${mon_host}&quot;':3300,v1:'&quot;${mon_host}&quot;':6789]&quot;, &quot;mon_initial_members&quot;: &quot;'&quot;${good_mon_id}&quot;'&quot;}}'

</code></pre><p>重启监控实例：</p>
<pre tabindex="0"><code>kubectl replace --force -f rook-ceph-mon-b-deployment.yaml

</code></pre><p>重启 operator:</p>
<pre tabindex="0"><code># create the operator. it is safe to ignore the errors that a number of resources already exist.
kubectl -n rook-ceph scale deployment rook-ceph-operator --replicas=1

</code></pre><h3 id="jenkins-挂载-pvc-应用的数据恢复">Jenkins 挂载 PVC 应用的数据恢复</h3>
<p>假设 Jenkins 数据损坏，想修复 Jenkins 的数据目录，可以采用把 PVC 挂载带临时镜像并配合 <code>kubectl cp</code> 实现，步骤如下。</p>
<p>\1. 获得当前 Jenkins 容器的运行权限：</p>
<pre tabindex="0"><code>$ kubectl --namespace=cje-cluster-example get pods cjoc-0 -o jsonpath='{.spec.securityContext}'
map[fsGroup:1000]

</code></pre><p>\2. 关闭容器：</p>
<pre tabindex="0"><code>$ kubectl --namespace=cje-cluster-example scale statefulset/cjoc --replicas=0
statefulset.apps &quot;cjoc&quot; scaled

</code></pre><p>\3. 查看 PVC：</p>
<pre tabindex="0"><code>$ kubectl --namespace=cje-cluster-example get pvc
NAME                  STATUS    VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS   AGE
jenkins-home-cjoc-0   Bound     pvc-6b27e963-b770-11e8-bcbf-42010a8400c1   20Gi       RWO            standard       46d
jenkins-home-mm1-0    Bound     pvc-b2b7e305-ba66-11e8-bcbf-42010a8400c1   50Gi       RWO            standard       42d
jenkins-home-mm2-0    Bound     pvc-6561b8da-c0c8-11e8-bcbf-42010a8400c1   50Gi       RWO            standard       34d

</code></pre><p>\4. 挂载 PVC 到临时镜像中方便恢复数据：</p>
<pre tabindex="0"><code>$ cat &lt;&lt;EOF | kubectl --namespace=cje-cluster-example create -f -
kind: Pod
apiVersion: v1
metadata:
  name: rescue-pod
spec:
  securityContext:
    runAsUser: 1000
    fsGroup: 1000
  volumes:
    - name: rescue-storage
      persistentVolumeClaim:
       claimName: jenkins-home-cjoc-0
  containers:
    - name: rescue-container
      image: nginx
      command: [&quot;/bin/sh&quot;]
      args: [&quot;-c&quot;, &quot;while true; do echo hello; sleep 10;done&quot;]
      volumeMounts:
        - mountPath: &quot;/tmp/jenkins-home&quot;
          name: rescue-storage
EOF
pod &quot;rescue-pod&quot; created

</code></pre><p>\5. 复制备份数据到临时镜像：</p>
<pre tabindex="0"><code>kubectl cp oc-jenkins-home.backup.tar.gz rescue-pod:/tmp/

</code></pre><p>\6. 解压数据到 PVC 挂载卷：</p>
<pre tabindex="0"><code>kubectl exec --namespace=cje-cluster-example rescue-pod -it -- tar -xzf /tmp/oc-jenkins-home.backup.tar.gz -C /tmp/jenkins-home

</code></pre><p>\7. 删除临时镜像 Pod：</p>
<pre tabindex="0"><code>kubectl --namespace=cje-cluster-example delete pod rescue-pod

</code></pre><p>\8. 恢复 Jenkins 容器：</p>
<pre tabindex="0"><code>kubectl --namespace=cje-cluster-example scale statefulset/cjoc --replicas=1

</code></pre><h3 id="kubernetes-集群的备份">Kubernetes 集群的备份</h3>
<p>Kubernetes 集群是分布式集群，我们备份集群的元数据的目的一般有两个主要目的：</p>
<ul>
<li>能快速恢复控制节点而不是计算节点</li>
<li>能恢复应用容器</li>
</ul>
<p>从集群备份的难度来讲，我们要清楚理解集群控制节点上有哪些关键数据是需要备份的：自签名证书、etcd 数据、kubeconfig。</p>
<p>拿单个控制几点服务器上的备份步骤来看：</p>
<pre tabindex="0"><code># Backup certificates
sudo cp -r /etc/kubernetes/pki backup/
# Make etcd snapshot
sudo docker run --rm -v $(pwd)/backup:/backup \
    --network host \
    -v /etc/kubernetes/pki/etcd:/etc/kubernetes/pki/etcd \
    --env ETCDCTL_API=3 \
    k8s.gcr.io/etcd:3.4.3-0 \
    etcdctl --endpoints=https://127.0.0.1:2379 \
    --cacert=/etc/kubernetes/pki/etcd/ca.crt \
    --cert=/etc/kubernetes/pki/etcd/healthcheck-client.crt \
    --key=/etc/kubernetes/pki/etcd/healthcheck-client.key \
    snapshot save /backup/etcd-snapshot-latest.db

# Backup kubeadm-config
sudo cp /etc/kubeadm/kubeadm-config.yaml backup/

</code></pre><p>数据恢复一个控制节点的操作如下：</p>
<pre tabindex="0"><code># Restore certificates
sudo cp -r backup/pki /etc/kubernetes/

# Restore etcd backup
sudo mkdir -p /var/lib/etcd
sudo docker run --rm \
    -v $(pwd)/backup:/backup \
    -v /var/lib/etcd:/var/lib/etcd \
    --env ETCDCTL_API=3 \
    k8s.gcr.io/etcd:3.4.3-0 \
    /bin/sh -c &quot;etcdctl snapshot restore '/backup/etcd-snapshot-latest.db' ; \
    mv /default.etcd/member/ /var/lib/etcd/&quot;

# Restore kubeadm-config
sudo mkdir /etc/kubeadm
sudo cp backup/kubeadm-config.yaml /etc/kubeadm/

# Initialize the master with backup
sudo kubeadm init --ignore-preflight-errors=DirAvailable--var-lib-etcd \
    --config /etc/kubeadm/kubeadm-config.yaml

</code></pre><p>通过以上案例知道 Kubernetes 集群中 etcd 数据的备份和恢复，学会善用和 <code>kubectl cp</code> 的配合使用。</p>
<h3 id="总结">总结</h3>
<p>依赖 Kubernetes 原生的数据复制能力 <code>kubectl cp</code> 和 cronjob，我们可以应对大部分的数据备份和恢复工作。当需要处理分布式系统的备份和恢复的时候，大部分情况并不是去备份数据，而是尝试从有效节点中去除故障节点，让集群能自愈。这是分布式系统的特点，它可以自愈。但是分布式系统的弱点也在于自愈是有条件的，如果故障节点超过可用节点数 Quorum，再智能也是无用的。所以备份仍然是最后一道防线。一定要做定期的并且冗余的<strong>数据备份</strong>。</p>
<h3 id="参考链接">参考链接</h3>
<ul>
<li><a href="https://github.com/rook/rook/blob/master/Documentation/ceph-disaster-recovery.md">https://github.com/rook/rook/blob/master/Documentation/ceph-disaster-recovery.md</a></li>
<li><a href="https://zh.wikipedia.org/wiki/Quorum_(%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F)">https://zh.wikipedia.org/wiki/Quorum_(%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F)</a></li>
</ul>

        </div>
    </article>
</div>

    </main>
    <footer>
        <div id="pagination">
            
<div class="paginator">
    <div class="prev">
        
        <a href="/documents/columns/kubernetes/kubernetes%E5%AE%9E%E8%B7%B5%E5%85%A5%E9%97%A8%E6%8C%87%E5%8D%97/22-%E5%AD%98%E5%82%A8%E5%AF%B9%E8%B1%A1-pvpvcstorage-classes-%E7%9A%84%E7%AE%A1%E7%90%86%E8%90%BD%E5%9C%B0%E5%AE%9E%E8%B7%B5/"><span>22 存储对象 PV、PVC、Storage Classes 的管理落地实践</span></a>
    </div>
    <div class="next">
        
        <a href="/documents/columns/kubernetes/kubernetes%E5%AE%9E%E8%B7%B5%E5%85%A5%E9%97%A8%E6%8C%87%E5%8D%97/24-%E7%BB%83%E4%B9%A0%E7%AF%87k8s-%E9%9B%86%E7%BE%A4%E9%85%8D%E7%BD%AE%E6%B5%8B%E9%AA%8C/"><span>24 练习篇：K8s 集群配置测验</span></a>
    </div>
</div>

        </div>
        <div class="toggler" onclick="toggleLight(this);">💡</div>
        <div id="copyright" style="display: none;">
    
    
    <p>&copy; 27270 <a href="/"></a>, powered by Hugo and Qiao</p>
</div>
    </footer>
    
<script>
    function toggleLight() {
        document.getElementById('page').classList.toggle('night');
    }
</script>
</body>

</html>